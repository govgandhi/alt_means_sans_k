{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "from itertools import cycle\n",
    "import warnings\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.cluster import OPTICS, DBSCAN\n",
    "from fastnode2vec import Graph, Node2Vec\n",
    "import faiss\n",
    "import lfr\n",
    "import embcom\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# warnings.resetwarnings() # To change it back (optional)\n",
    "\n",
    "sys.path.append(\"/nobackup/gogandhi/alt_means_sans_k/\")\n",
    "\n",
    "from scripts.similarity_scores import *\n",
    "from scripts.nets_and_embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"N\": 1000,\n",
    "    \"k\": 50,\n",
    "    \"maxk\": 1000,\n",
    "    \"minc\": 50,\n",
    "    \"maxc\": 1000, #maxc > maxk and minc > mink has to be satisfied.\n",
    "    \"tau\": 3.0,\n",
    "    \"tau2\": 1.2,\n",
    "    \"mu\": 0.20,\n",
    "}\n",
    "\n",
    "emb_params = {\n",
    "    \"method\": \"node2vec\",\n",
    "    \"window_length\": 10,\n",
    "    \"walk_length\": 80,\n",
    "    \"num_walks\": 10,\n",
    "    \"dim\": 64,\n",
    "}\n",
    "\n",
    "score_keys = ['kmeans', 'xmeans', 'infomap','proposed']\n",
    "device_name=\"cuda:3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, community_table, emb = create_and_save_network_and_embedding(params, emb_params, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "6    2\n",
      "7    3\n",
      "8    1\n",
      "9    1\n",
      "Name: community_id, dtype: int64\n",
      "1000 [0 0 0 0 0 0 1 2 0 0]\n",
      "Element-centric similarity:\n",
      "K-means: 0.9999999999999997\n"
     ]
    }
   ],
   "source": [
    "# Need net, node_table and emb files\n",
    "# net is G, emb files are wv, node_table probably\n",
    "# has some info about ground truth community labels or smth like that.\n",
    "\n",
    "# Define a function that calculates element-centric similarity:\n",
    "def calc_esim(y, ypred):\n",
    "\n",
    "    ylab, y = np.unique(y, return_inverse=True)\n",
    "    ypredlab, ypred = np.unique(ypred, return_inverse=True)\n",
    "    \n",
    "    Ka, Kb = len(ylab), len(ypredlab)\n",
    "    K = np.maximum(Ka, Kb) # Max # of communities\n",
    "    N = len(y) # # of nodes\n",
    "    \n",
    "    UA = sparse.csr_matrix((np.ones_like(y), (np.arange(y.size), y)), shape=(N,K))\n",
    "    UB = sparse.csr_matrix((np.ones_like(ypred), (np.arange(ypred.size), ypred)), shape=(N, K))    \n",
    "    \n",
    "    nA = np.array(UA.sum(axis=0)).reshape(-1)\n",
    "    nB = np.array(UB.sum(axis=0)).reshape(-1)\n",
    "\n",
    "# nAB[i][j] is read as the number of elements that belong to ith ground truth label and jth predicrted label.\n",
    "# nAB[1][0] = 1 For ground truth label with index 1 and predicted label 0 we have 1 element. i.e. 0000|1| vs 1110|0|\n",
    "\n",
    "    nAB = (UA.T @ UB).toarray()\n",
    "    nAB_rand = np.outer(nA, nB) / N\n",
    "    \n",
    "# assuming that each element has an equal probability of being assigned to any label,\n",
    "# and the expected counts are calculated based on label frequencies.\n",
    "\n",
    "\n",
    "    # Calc element-centric similarity\n",
    "    Q = np.maximum(nA[:, None] @ np.ones((1, K)), np.ones((K, 1)) @ nB[None, :]) \n",
    "    Q = 1 / np.maximum(Q, 1)\n",
    "    S = np.sum(np.multiply(Q, (nAB**2))) / N\n",
    "    \n",
    "    # Calc the expected element-centric similarity for random partitions\n",
    "    Q = np.maximum(nA[:, None] @ np.ones((1, K)), np.ones((K, 1)) @ nB[None, :]) \n",
    "    Q = 1 / np.maximum(Q, 1)\n",
    "    Srand = np.sum(np.multiply(Q, (nAB_rand**2))) / N\n",
    "    Scorrected = (S - Srand) / (1 - Srand)\n",
    "    return Scorrected\n",
    "\n",
    "\n",
    "#wv_array = np.vstack([wv[node] for node in list(G.nodes())])\n",
    "\n",
    "# Normalize the vector of each node to have unit length. This normalization improves clustering.\n",
    "X = np.einsum(\"ij,i->ij\", emb, 1 / np.maximum(np.linalg.norm(emb, axis=1), 1e-24))\n",
    "X = emb.copy()\n",
    "# Clustering\n",
    "kmeans = KMeans(n_clusters= len(set(community_table[\"community_id\"])), random_state=0).fit(X)\n",
    "\n",
    "\n",
    "# Evaluate the clustering\n",
    "score_kmeans = calc_esim(community_table[\"community_id\"], kmeans.labels_)\n",
    "\n",
    "\n",
    "print(f\"Element-centric similarity:\") \n",
    "print(f\"K-means: {score_kmeans}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knn_edges(emb, num_neighbors, \n",
    "                   target=None, metric=\"dotsim\",\n",
    "                   device=None):\n",
    "    \n",
    "    k = int(np.minimum(num_neighbors + 1, emb.shape[0]).astype(int))\n",
    "    indices, distances = find_knn(emb if target is None else target, emb,\n",
    "                                  num_neighbors=k,\n",
    "                                  metric=metric,\n",
    "                                  device=device)\n",
    "    r = np.outer(np.arange(indices.shape[0]), np.ones((1, indices.shape[1]))).astype(int)\n",
    "    r, c, distances = (r.reshape(-1),indices.astype(int).reshape(-1),distances.reshape(-1))\n",
    "    if len(r) == 0:\n",
    "        return r, c, distances \n",
    "    \n",
    "    return r, c, distances\n",
    "\n",
    "\n",
    "def find_knn(target, emb, num_neighbors, metric=\"dotsim\", device=None): \n",
    "    if metric == \"dotsim\":\n",
    "        index = faiss.IndexFlatIP(emb.shape[1]) \n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(emb.shape[1]) \n",
    "    \n",
    "    if device is None:\n",
    "        index.add(emb.astype(np.float32))\n",
    "        distances, indices = index.search(target.astype(np.float32), k=num_neighbors)\n",
    "        # This line takes too long to load.\n",
    "    else: \n",
    "        try:\n",
    "            gpu_id = int(device[-1])\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, gpu_id, index)\n",
    "            index.add(emb.astype(np.float32))\n",
    "            distances, indices = index.search(\n",
    "                target.astype(np.float32), k=num_neighbors\n",
    "            )\n",
    "        except RuntimeError:\n",
    "            if metric == \"dotsim\":\n",
    "                index = faiss.IndexFlatIP(emb.shape[1]) \n",
    "            else:\n",
    "                index = faiss.IndexFlatL2(emb.shape[1])\n",
    "            \n",
    "            index.add(emb.astype(np.float32))\n",
    "            distances, indices = index.search(target.astype(np.float32),\n",
    "                                              k=num_neighbors)\n",
    "    return indices, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpos, cpos, vpos = find_knn_edges(emb, num_neighbors=100, device = \"cuda:3\")\n",
    "cneg = np.random.choice(emb.shape[0], len(cpos))\n",
    "vneg = np.array(np.sum(emb[rpos, :] * emb[cneg, :], axis=1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(\n",
    "    np.concatenate([vpos, vneg]).reshape((-1, 1)),\n",
    "    np.concatenate([np.ones_like(vpos), np.zeros_like(vneg)]),\n",
    ")\n",
    "w1, b0 = model.coef_[0, 0], -model.intercept_[0] \n",
    "print(f\"w0={w1}, b1={b0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "def louvain(Z, w1, b0, num_neighbors=100, iteration = 50, device = \"cuda:0\", return_member_matrix=False):\n",
    "    \"\"\"\"Louvain algorithm for vecto data\n",
    "    :param Z: embedding\n",
    "    :type Z: numpy.ndarray\n",
    "    :param w1: regression coefficient\n",
    "    :type w1: float\n",
    "    :param b0: intercept\n",
    "    :type b0: float\n",
    "    :param num_neighbors: Number of neighbors, defaults to 100\n",
    "    :type num_neighbors: int, optional\n",
    "    :param iteration: The maximum number of iterations, defaults to 50\n",
    "    :type iteration: int, optional\n",
    "    :param device: _description_, defaults to \"cuda:0\"\n",
    "    :type device: str, optional\n",
    "    :param return_member_matrix: _description_, defaults to False\n",
    "    :type return_member_matrix: bool, optional\n",
    "    :return: _description_\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    # Initialize the intermediate variables\n",
    "    num_nodes = Z.shape[0]\n",
    "    node_size = np.ones(num_nodes)\n",
    "    U = sparse.identity(num_nodes, format=\"csr\")\n",
    "    Vt = Z.copy()\n",
    "    \n",
    "    # The main loop for the Louvain algorithm\n",
    "    while True:\n",
    "        # Find the community assignment for the given graph # using a label switching algorithm\n",
    "        cids_t = label_switching(\n",
    "               Z=Vt,\n",
    "               num_neighbors=num_neighbors,\n",
    "               rho=b0/w1,\n",
    "               node_size=node_size,\n",
    "               epochs=iteration,\n",
    "               device=device,\n",
    "                )\n",
    "        \n",
    "        # This is to make the community labels continuous integer variables\n",
    "        _, cids_t = np.unique(cids_t, return_inverse=True)\n",
    "        \n",
    "        # If no merging, we are good to go out from the loop\n",
    "        if int(max(cids_t) + 1) == Vt.shape[0]: \n",
    "            break\n",
    "            \n",
    "        # If two nodes are merged, we created an aggregated network, \n",
    "        #where a node represents a community.\n",
    "        \n",
    "        num_nodes_t = len(cids_t)\n",
    "        k = int(np.max(cids_t) + 1)\n",
    "        Ut = sparse.csr_matrix((np.ones(num_nodes_t), (np.arange(num_nodes_t), cids_t)), shape=(num_nodes_t, k))\n",
    "        U = U @ Ut\n",
    "        Vt = Ut.T @ Vt\n",
    "        \n",
    "        node_size = np.array(Ut.T @ node_size).reshape(-1)\n",
    "    if return_member_matrix: \n",
    "        return U\n",
    "    cids = np.array((U @ sparse.diags(np.arange(U.shape[1]))).sum(axis=1)).reshape(-1)\n",
    "\n",
    "    return cids\n",
    "\n",
    "#\n",
    "# Clustering based on a label switching algorithm\n",
    "#\n",
    "def label_switching(Z, rho, num_neighbors=50, node_size=None, device=None,epochs=50):\n",
    "    num_nodes, dim = Z.shape\n",
    "    if node_size is None:\n",
    "        node_size = np.ones(num_nodes)\n",
    "    Z = Z.copy(order=\"C\").astype(np.float32)\n",
    "    # Construct the candidate graph\n",
    "    Z1 = np.hstack([Z, np.ones((num_nodes, 1))])\n",
    "    Zrho = np.hstack([Z, -rho * node_size.reshape((-1, 1))])\n",
    "\n",
    "    r, c, v = find_knn_edges(\n",
    "        Zrho,\n",
    "        target=Z1,\n",
    "        num_neighbors=num_neighbors,\n",
    "        metric=\"cosine\",\n",
    "        device=device)\n",
    "    A = sparse.csr_matrix((v, (r, c)), shape=(num_nodes, num_nodes))\n",
    "    \n",
    "    return _label_switching_(\n",
    "        A_indptr=A.indptr,\n",
    "        A_indices=A.indices,\n",
    "        Z=Z,\n",
    "        num_nodes=num_nodes,\n",
    "        rho=rho,\n",
    "        node_size=node_size,\n",
    "        epochs=epochs)\n",
    "\n",
    "#@numba.jit(nopython=True, cache=True)\n",
    "def _label_switching_(A_indptr, A_indices, Z, num_nodes, rho, node_size,epochs=100):\n",
    "    Nc = np.zeros(num_nodes)\n",
    "    cids = np.arange(num_nodes)\n",
    "    Vc = Z.copy()\n",
    "    Vnorm = np.sum(np.multiply(Z, Z), axis=1).reshape(-1) \n",
    "    for nid in range(num_nodes):\n",
    "            Nc[nid] += node_size[nid]\n",
    "    for _it in range(epochs):\n",
    "        order = np.random.choice(num_nodes, size=num_nodes, replace=False) \n",
    "        updated_node_num = 0\n",
    "        \n",
    "        for _k, node_id in enumerate(order):\n",
    "            # Get the weight and normalized weight\n",
    "            neighbors = A_indices[A_indptr[node_id] : A_indptr[node_id + 1]]\n",
    "\n",
    "            # Calculate the grain\n",
    "            c = cids[node_id]\n",
    "            clist = np.unique(cids[neighbors])\n",
    "            next_cid = -1\n",
    "            dqmax = 0\n",
    "            qself = (\n",
    "                np.sum(Z[node_id, :] * Vc[c, :])\n",
    "                - Vnorm[node_id]\n",
    "                - rho * node_size[node_id] * (Nc[c] - node_size[node_id]))\n",
    "\n",
    "            for cprime in clist:\n",
    "                if c == cprime: \n",
    "                    continue\n",
    "                dq = (np.sum(Z[node_id, :] * Vc[cprime, :])\n",
    "                        - rho * node_size[node_id] * Nc[cprime]) - qself\n",
    "                if dqmax < dq:\n",
    "                    next_cid = cprime\n",
    "                    dqmax = dq\n",
    "            if dqmax <= 1e-16: \n",
    "                continue\n",
    "\n",
    "            Nc[c] -= node_size[node_id]\n",
    "            Nc[next_cid] += node_size[node_id]\n",
    "\n",
    "            Vc[c, :] -= Z[node_id, :]\n",
    "            Vc[next_cid, :] += Z[node_id, :]\n",
    "\n",
    "            cids[node_id] = next_cid\n",
    "            updated_node_num += 1\n",
    "\n",
    "        if (updated_node_num / np.maximum(1, num_nodes)) < 1e-3: \n",
    "            break\n",
    "    return cids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = louvain(emb, w1, b0, device = \"cuda:3\")\n",
    "score_proposed = calc_esim(community_table[\"community_id\"], gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "print(f\"Element-centric similarity:\") \n",
    "print(f\"K-means: {score_kmeans}\")\n",
    "print(f\"DBSCAN: {score_dbscan}\") # Something Wrong\n",
    "print(f\"OPTICS: {score_optics}\") # Something Wrong\n",
    "print(f\"Proposed: {score_proposed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "clus_sz = np.unique(community_table[\"community_id\"], return_counts=True)[1] \n",
    "clus_sz_proposed = np.unique(gs, return_counts = True)[1]\n",
    "clus_sz_kmeans = np.unique(kmeans.labels_, return_counts=True)[1]\n",
    "clus_sz_dbscan = np.unique(dbscan.labels_, return_counts=True)[1]\n",
    "clus_sz_optics = np.unique(optics.labels_, return_counts=True)[1]\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "sns.histplot(clus_sz_proposed, bins = 30, color = sns.color_palette().as_hex()[0],label = \"Proposed\", ax = ax)\n",
    "sns.histplot(clus_sz_kmeans, bins = 30, color = sns.color_palette().as_hex()[1],label = \"K-means\", ax = ax)\n",
    "#sns.histplot(clus_sz_dbscan, bins = 30, color = sns.color_palette().as_hex()[2],label = \"DBSCAN\", ax = ax)\n",
    "#sns.histplot(clus_sz_optics, bins = 30, color = sns.color_palette().as_hex()[3],label = \"OPTICS\", ax = ax)\n",
    "sns.histplot(clus_sz, bins = 30, color = sns.color_palette().as_hex()[4], label= \"Ground-truth\", ax = ax) \n",
    "ax.legend(frameon = False) \n",
    "ax.set_xlabel(\"Cluster Size\")\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmeans_env",
   "language": "python",
   "name": "kmeans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
