{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6bd449-ba10-4924-b221-40c375f97b22",
   "metadata": {},
   "source": [
    "How do we connect theoretical detectability limit with the heuristic we use to get logistic regression parameters?\n",
    "\n",
    "The # of edges between comms vs within communities in Y_ij can be used to connect it to theoretical detectability limit?\n",
    "We know the theoretical limit of the PPM, so can we do tests to match it and use that as a proxy for lfr nets?\n",
    "\n",
    "Detectability limit of normalized Laplacian matrix generated by PPM is mu* = 1- 1/sqrt(<k>), and mu = n * p_out/<k>\n",
    "\n",
    "Use SBM nets, Laplacian eigenmaps, see how different the Y_ijs and eventually the elem centric sim changes with respect to increasing mixing in SBMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79fb7ff-9fe8-4534-93cb-0db20b9b39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the machinery going\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "#import os\n",
    "#import networkx as nx\n",
    "#import gensim\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.cluster import OPTICS, DBSCAN\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import faiss\n",
    "import fast_hdbscan\n",
    "import embcom\n",
    "\n",
    "#import lfr\n",
    "#import embcom\n",
    "#import csv\n",
    "import sys\n",
    "sys.path.append(\"/nobackup/gogandhi/alt_means_sans_k/\")\n",
    "\n",
    "from scripts.nets_and_embeddings import create_and_save_network_and_embedding\n",
    "#from scripts.clustering_methods import clustering_method_values\n",
    "\n",
    "from pyclustering.cluster import cluster_visualizer\n",
    "from pyclustering.cluster.xmeans import xmeans, splitting_type\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.utils import read_sample\n",
    "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
    "import numpy as np\n",
    "import belief_propagation\n",
    "import infomap\n",
    "\n",
    "from graph_tool.all import Graph,minimize_blockmodel_dl\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse, stats\n",
    "from scipy.sparse.csgraph import connected_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91493f26-c4af-44af-907b-310c6c0a931e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82930e-a87c-45f2-a68e-13708e694cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b81baf-5523-4002-82be-aac100068fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates element-centric similarity:\n",
    "def calc_esim(y, ypred):\n",
    "\n",
    "    ylab, y = np.unique(y, return_inverse=True)\n",
    "    ypredlab, ypred = np.unique(ypred, return_inverse=True)\n",
    "    \n",
    "    Ka, Kb = len(ylab), len(ypredlab)\n",
    "    K = np.maximum(Ka, Kb)\n",
    "    N = len(y)\n",
    "    \n",
    "    UA = sparse.csr_matrix((np.ones_like(y), (np.arange(y.size), y)), shape=(N,K))\n",
    "    UB = sparse.csr_matrix((np.ones_like(ypred), (np.arange(ypred.size), ypred)), shape=(N, K))    \n",
    "    \n",
    "    nA = np.array(UA.sum(axis=0)).reshape(-1)\n",
    "    nB = np.array(UB.sum(axis=0)).reshape(-1)\n",
    "\n",
    "# nAB[i][j] is read as the number of elements that belong to ith ground truth label and jth predicrted label.\n",
    "# nAB[1][0] = 1 For ground truth label with index 1 and predicted label 0 we have 1 element. i.e. 0000|1| vs 1110|0|\n",
    "\n",
    "    nAB = (UA.T @ UB).toarray()\n",
    "    nAB_rand = np.outer(nA, nB) / N\n",
    "    \n",
    "# assuming that each element has an equal probability of being assigned to any label,\n",
    "# and the expected counts are calculated based on label frequencies.\n",
    "\n",
    "\n",
    "    # Calc element-centric similarity\n",
    "    Q = np.maximum(nA[:, None] @ np.ones((1, K)), np.ones((K, 1)) @ nB[None, :]) \n",
    "    Q = 1 / np.maximum(Q, 1)\n",
    "    S = np.sum(np.multiply(Q, (nAB**2))) / N\n",
    "    \n",
    "    # Calc the expected element-centric similarity for random partitions\n",
    "    #Q = np.maximum(nA[:, None] @ np.ones((1, K)), np.ones((K, 1)) @ nB[None, :]) \n",
    "    #Q = 1 / np.maximum(Q, 1)\n",
    "    Srand = np.sum(np.multiply(Q, (nAB_rand**2))) / N\n",
    "    Scorrected = (S - Srand) / (1 - Srand)\n",
    "    return Scorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13362466-33c9-4424-bafc-ebe52b282b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knn_edges(emb, num_neighbors, \n",
    "                   target=None, metric=\"dotsim\",\n",
    "                   device=None):\n",
    "    \n",
    "    k = int(np.minimum(num_neighbors + 1, emb.shape[0]).astype(int))\n",
    "    indices, distances = find_knn(emb if target is None else target, emb,\n",
    "                                  num_neighbors=k,\n",
    "                                  metric=metric,\n",
    "                                  device=device)\n",
    "    r = np.outer(np.arange(indices.shape[0]), np.ones((1, indices.shape[1]))).astype(int)\n",
    "    r, c, distances = (r.reshape(-1),indices.astype(int).reshape(-1),distances.reshape(-1))\n",
    "    if len(r) == 0:\n",
    "        return r, c, distances \n",
    "    \n",
    "    return r, c, distances\n",
    "\n",
    "# Only place where GPU is used, need to handle its closing. \n",
    "def find_knn(target, emb, num_neighbors, metric=\"dotsim\", device=None): \n",
    "    if metric == \"dotsim\":\n",
    "        index = faiss.IndexFlatIP(emb.shape[1]) \n",
    "    elif metric == \"euclidean\":\n",
    "        index = faiss.IndexFlatL2(emb.shape[1])\n",
    "    elif metric == \"manhattan\":\n",
    "        index = faiss.IndexFlatL1(emb.shape[1])\n",
    "    elif metric == \"cosine\":\n",
    "        index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    elif metric==\"mahalanobis\":\n",
    "        # THis mathematical trick works, but it needs some reworking in target.astype to\n",
    "        # give right results.\n",
    "        # map the vectors back to a space where they follow a unit Gaussian\n",
    "        xc = emb - emb.mean(0)\n",
    "        cov = np.dot(xc.T, xc) / xc.shape[0]\n",
    "        L = np.linalg.cholesky(cov)\n",
    "        mahalanobis_transform = np.linalg.inv(L)\n",
    "        emb = np.dot(emb, mahalanobis_transform.T)\n",
    "        index = faiss.IndexFlatL2(emb.shape[1])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric specified.\")\n",
    "    \n",
    "    if device is None:\n",
    "        index.add(emb.astype(np.float32))\n",
    "        distances, indices = index.search(target.astype(np.float32), k=num_neighbors)\n",
    "        # This line takes too long to load.\n",
    "    else: \n",
    "        try:\n",
    "            gpu_id = int(device[-1])\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, gpu_id, index)\n",
    "            index.add(emb.astype(np.float32))\n",
    "            distances, indices = index.search(\n",
    "                target.astype(np.float32), k=num_neighbors\n",
    "            )\n",
    "        except RuntimeError:\n",
    "            if metric == \"dotsim\":\n",
    "                index = faiss.IndexFlatIP(emb.shape[1]) \n",
    "            elif metric == \"euclidean\":\n",
    "                index = faiss.IndexFlatL2(emb.shape[1])\n",
    "            elif metric == \"manhattan\":\n",
    "                index = faiss.IndexFlatL1(emb.shape[1])\n",
    "            elif metric == \"cosine\":\n",
    "                index = faiss.IndexFlatIP(emb.shape[1])\n",
    "                emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid metric specified.\")\n",
    "            \n",
    "            index.add(emb.astype(np.float32))\n",
    "            distances, indices = index.search(target.astype(np.float32),\n",
    "                                              k=num_neighbors)\n",
    "        \n",
    "    index.reset()\n",
    "    return indices, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4692b063-6367-4d40-abe4-d054f72a9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "def louvain(Z, w1, b0, num_neighbors=10, iteration = 50, device = \"cuda:0\", return_member_matrix=False):\n",
    "    \"\"\"\"Louvain algorithm for vecto data\n",
    "    :param Z: embedding\n",
    "    :type Z: numpy.ndarray\n",
    "    :param w1: regression coefficient\n",
    "    :type w1: float\n",
    "    :param b0: intercept\n",
    "    :type b0: float\n",
    "    :param num_neighbors: Number of neighbors, defaults to 100\n",
    "    :type num_neighbors: int, optional\n",
    "    :param iteration: The maximum number of iterations, defaults to 50\n",
    "    :type iteration: int, optional\n",
    "    :param device: _description_, defaults to \"cuda:0\"\n",
    "    :type device: str, optional\n",
    "    :param return_member_matrix: _description_, defaults to False\n",
    "    :type return_member_matrix: bool, optional\n",
    "    :return: _description_\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    # Initialize the intermediate variables\n",
    "    num_nodes = Z.shape[0]\n",
    "    node_size = np.ones(num_nodes)\n",
    "    U = sparse.identity(num_nodes, format=\"csr\")\n",
    "    Vt = Z.copy()\n",
    "    \n",
    "    # The main loop for the Louvain algorithm\n",
    "    while True:\n",
    "        # Find the community assignment for the given graph # using a label switching algorithm\n",
    "        cids_t = label_switching(\n",
    "               Z=Vt,\n",
    "               num_neighbors=num_neighbors,\n",
    "               rho=b0/w1,\n",
    "               node_size=node_size,\n",
    "               epochs=iteration,\n",
    "               device=device,\n",
    "                )\n",
    "        \n",
    "        # This is to make the community labels continuous integer variables\n",
    "        _, cids_t = np.unique(cids_t, return_inverse=True)\n",
    "        \n",
    "        # If no merging, we are good to go out from the loop\n",
    "        if int(max(cids_t) + 1) == Vt.shape[0]: \n",
    "            break\n",
    "            \n",
    "        # If two nodes are merged, we created an aggregated network, \n",
    "        #where a node represents a community.\n",
    "        \n",
    "        num_nodes_t = len(cids_t)\n",
    "        k = int(np.max(cids_t) + 1)\n",
    "        Ut = sparse.csr_matrix((np.ones(num_nodes_t), (np.arange(num_nodes_t), cids_t)), shape=(num_nodes_t, k))\n",
    "        U = U @ Ut\n",
    "        Vt = Ut.T @ Vt\n",
    "        \n",
    "        node_size = np.array(Ut.T @ node_size).reshape(-1)\n",
    "    if return_member_matrix: \n",
    "        return U\n",
    "    cids = np.array((U @ sparse.diags(np.arange(U.shape[1]))).sum(axis=1)).reshape(-1)\n",
    "\n",
    "    return cids\n",
    "\n",
    "#\n",
    "# Clustering based on a label switching algorithm\n",
    "#\n",
    "def label_switching(Z, rho, num_neighbors=50, node_size=None, device=None,epochs=50): # This involves distance metrics (cosine similarity, atm)\n",
    "    num_nodes, dim = Z.shape\n",
    "    if node_size is None:\n",
    "        node_size = np.ones(num_nodes)\n",
    "    Z = Z.copy(order=\"C\").astype(np.float32)\n",
    "    # Construct the candidate graph\n",
    "    Z1 = np.hstack([Z, np.ones((num_nodes, 1))])\n",
    "    Zrho = np.hstack([Z, -rho * node_size.reshape((-1, 1))])\n",
    "\n",
    "    r, c, v = find_knn_edges(\n",
    "        Zrho,\n",
    "        target=Z1,\n",
    "        num_neighbors=num_neighbors,\n",
    "        metric=\"cosine\",\n",
    "        device=device)\n",
    "    A = sparse.csr_matrix((v, (r, c)), shape=(num_nodes, num_nodes))\n",
    "    \n",
    "    return _label_switching_(\n",
    "        A_indptr=A.indptr,\n",
    "        A_indices=A.indices,\n",
    "        Z=Z,\n",
    "        num_nodes=num_nodes,\n",
    "        rho=rho,\n",
    "        node_size=node_size,\n",
    "        epochs=epochs)\n",
    "\n",
    "#@numba.jit(nopython=True, cache=True)\n",
    "def _label_switching_(A_indptr, A_indices, Z, num_nodes, rho, node_size,epochs=100):\n",
    "    Nc = np.zeros(num_nodes)\n",
    "    cids = np.arange(num_nodes)\n",
    "    Vc = Z.copy()\n",
    "    Vnorm = np.sum(np.multiply(Z, Z), axis=1).reshape(-1) \n",
    "    for nid in range(num_nodes):\n",
    "            Nc[nid] += node_size[nid]\n",
    "    for _it in range(epochs):\n",
    "        order = np.random.choice(num_nodes, size=num_nodes, replace=False) \n",
    "        updated_node_num = 0\n",
    "        \n",
    "        for _k, node_id in enumerate(order):\n",
    "            # Get the weight and normalized weight\n",
    "            neighbors = A_indices[A_indptr[node_id] : A_indptr[node_id + 1]]\n",
    "\n",
    "            # Calculate the grain\n",
    "            c = cids[node_id]\n",
    "            clist = np.unique(cids[neighbors])\n",
    "            next_cid = -1\n",
    "            dqmax = 0\n",
    "            qself = (\n",
    "                np.sum(Z[node_id, :] * Vc[c, :])\n",
    "                - Vnorm[node_id]\n",
    "                - rho * node_size[node_id] * (Nc[c] - node_size[node_id]))\n",
    "\n",
    "            for cprime in clist:\n",
    "                if c == cprime: \n",
    "                    continue\n",
    "                dq = (np.sum(Z[node_id, :] * Vc[cprime, :])\n",
    "                        - rho * node_size[node_id] * Nc[cprime]) - qself\n",
    "                if dqmax < dq:\n",
    "                    next_cid = cprime\n",
    "                    dqmax = dq\n",
    "            if dqmax <= 1e-16: \n",
    "                continue\n",
    "\n",
    "            Nc[c] -= node_size[node_id]\n",
    "            Nc[next_cid] += node_size[node_id]\n",
    "\n",
    "            Vc[c, :] -= Z[node_id, :]\n",
    "            Vc[next_cid, :] += Z[node_id, :]\n",
    "\n",
    "            cids[node_id] = next_cid\n",
    "            updated_node_num += 1\n",
    "\n",
    "        if (updated_node_num / np.maximum(1, num_nodes)) < 1e-3: \n",
    "            break\n",
    "    return cids\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af80ffa-888e-4b2c-9f18-e7fd3a238adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0cce8d-613a-43be-9e19-2a40eded56f3",
   "metadata": {},
   "source": [
    "### Get PPM network and generate embeddings using laplacian eigenmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a759c8b-6611-47a3-abf0-51febe2d86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating PPM code\n",
    "\n",
    "# n = 10000 # 100000\n",
    "# K = 64\n",
    "# cave = 10\n",
    "# mu = 0.4\n",
    "# #mu = np.round(np.linspace(0.05, 1, 20), 2)\n",
    "\n",
    "output_file = \"\"\n",
    "\n",
    "\n",
    "def generate_network(Cave, mixing_rate, N, q):\n",
    "    memberships = np.sort(np.arange(N) % q)\n",
    "\n",
    "    q = int(np.max(memberships) + 1)\n",
    "    N = len(memberships)\n",
    "    U = sparse.csr_matrix((np.ones(N), (np.arange(N), memberships)), shape=(N, q))\n",
    "\n",
    "    Cout = np.maximum(1, mixing_rate * Cave)\n",
    "    Cin = q * Cave - (q - 1) * Cout\n",
    "    pout = Cout / N\n",
    "    pin = Cin / N\n",
    "\n",
    "    Nk = np.array(U.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    P = np.ones((q, q)) * pout + np.eye(q) * (pin - pout)\n",
    "    probs = np.diag(Nk) @ P @ np.diag(Nk)\n",
    "    gt_params = {\n",
    "        \"b\": memberships,\n",
    "        \"probs\": probs,\n",
    "        \"micro_degs\": False,\n",
    "        \"in_degs\": np.ones_like(memberships) * Cave,\n",
    "        \"out_degs\": np.ones_like(memberships) * Cave,\n",
    "    }\n",
    "\n",
    "    # Generate the network until the degree sequence\n",
    "    # satisfied the thresholds\n",
    "    while True:\n",
    "        g = gt.generate_sbm(**gt_params)\n",
    "\n",
    "        A = gt.adjacency(g).T\n",
    "\n",
    "        A.data = np.ones_like(A.data)\n",
    "        # check if the graph is connected\n",
    "        if connected_components(A)[0] == 1:\n",
    "            break\n",
    "        break\n",
    "    return A, memberships, pd.DataFrame({\"community_id\": memberships})\n",
    "\n",
    "\n",
    "\n",
    "#A, memberships = generate_network(cave, mu, n, K)\n",
    "\n",
    "\n",
    "# sparse.save_npz(output_file, A)\n",
    "# node_ids = np.arange(A.shape[0]).astype(int)\n",
    "# pd.DataFrame({\"node_id\": node_ids, \"membership\": memberships}).to_csv(\n",
    "#     output_node_file, index=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d1e94ef-140d-4ccf-95fa-b1a185a9d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(1e4)\n",
    "K = 50\n",
    "cave = 50\n",
    "mu_vals = np.round(np.linspace(0.05, 1, 20), 2)\n",
    "mu=0.7\n",
    "A, memberships, community_table = generate_network(cave, mu, n, K)\n",
    "\n",
    "model = embcom.embeddings.LaplacianEigenMap()\n",
    "model.fit(A)\n",
    "emb = model.transform(dim=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e30a2-04a3-4be8-a337-535a31b9934c",
   "metadata": {},
   "source": [
    "How to see how different the Y_ijs are, how the edges between communities and within communities change?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3165d041-003c-42d9-b8f4-7a511e8bece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the clustering\n",
    "#def clustering_method_values(net, community_table, emb, score_keys, device_name):\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the vector of each node to have unit length. This normalization improves clustering.\n",
    "X = np.einsum(\"ij,i->ij\", emb, 1 / np.maximum(np.linalg.norm(emb, axis=1), 1e-24))\n",
    "#X = emb.copy()\n",
    "# Clustering\n",
    "\n",
    "#def proposed_method_labels(emb,device_name):\n",
    "\n",
    "rpos, cpos, vpos = find_knn_edges(emb, num_neighbors=100, device = \"cuda:1\")\n",
    "cneg = np.random.choice(emb.shape[0], len(cpos))\n",
    "vneg = np.array(np.sum(emb[rpos, :] * emb[cneg, :], axis=1)).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1a5e60-fa17-46df-ab04-e3a8a910607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting info about between and withing community edges of our k nn sampling\n",
    "\n",
    "comlist = []\n",
    "for i,j in zip(rpos, cpos):\n",
    "    if i<j:\n",
    "        comlist.append((i,j))\n",
    "    else:\n",
    "        comlist.append((j,i))\n",
    "\n",
    "neglist=[]\n",
    "for i,j in zip(rpos, cneg):\n",
    "    if i<j:\n",
    "        neglist.append((i,j))\n",
    "    else:\n",
    "        neglist.append((j,i))        \n",
    "\n",
    "Y_edges = (set(comlist) - set(neglist))\n",
    "\n",
    "within_community_edge = 0\n",
    "between_community_edge = 0\n",
    "\n",
    "for i in Y_edges:\n",
    "    if memberships[i[0]] == memberships[i[1]]:\n",
    "        within_community_edge+=1\n",
    "    else:\n",
    "        between_community_edge+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7838e441-c4db-4c7f-b3d3-e0bfa26ac533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315.52263736263734"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "within_community_edge/between_community_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367bb23f-d387-4dc4-a24e-25103e42c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(\n",
    "    np.concatenate([vpos, vneg]).reshape((-1, 1)),\n",
    "    np.concatenate([np.ones_like(vpos), np.zeros_like(vneg)]),\n",
    "        )\n",
    "w1, b0 = model.coef_[0, 0], -model.intercept_[0] \n",
    "proposed_method_labels = louvain(emb, w1, b0, device =  \"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81e434c7-3485-43ba-a81c-588e7d2e46ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349704000775639"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_esim(community_table[\"community_id\"], proposed_method_labels)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148cefe-f0f9-4f19-b24a-4fc50754081f",
   "metadata": {},
   "source": [
    "It jumps from suddenly at 0.6 to 1.059, if we increase the size of the network or if we change the number of neighbors, does this jump become less abrupt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdfbfa5-d496-4cae-a92b-4a92e9634ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how parameters n, k and b/w ratio affect detectability. Can we get them to match detectability limi  1 - 1/sqrt(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d345900-8c76-41c9-a157-5912b6bbf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using emb vs X, does it make it better? If yes, then we should definitely potts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b6902-0f7b-47bc-ad4e-75e42d85314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomorrow test new potts model like clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff341eb3-50cc-4447-a069-c9e9216a2d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmeans_env",
   "language": "python",
   "name": "kmeans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
