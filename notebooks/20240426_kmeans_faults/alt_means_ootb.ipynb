{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a90b84-39ef-4e11-859e-425856c81e06",
   "metadata": {},
   "source": [
    "This is going to be an out of the box version of alt_means that can be easily edited, changed parameters and played with easily when using for other things in other places.\n",
    "pip install ipynb and use the code: from ipynb.fs.full.alt_means_ootb import alt_means_ootb to get a function that returns labels, and element centric sim with ground truth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fa3535-3019-4f71-8ba4-bfa6b997d5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clustering_method_values' from 'scripts.clustering_methods' (/nobackup/gogandhi/alt_means_sans_k/scripts/clustering_methods.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/nobackup/gogandhi/alt_means_sans_k/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnets_and_embeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_and_save_network_and_embedding\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclustering_methods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clustering_method_values\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster_visualizer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyclustering\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxmeans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xmeans, splitting_type\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'clustering_method_values' from 'scripts.clustering_methods' (/nobackup/gogandhi/alt_means_sans_k/scripts/clustering_methods.py)"
     ]
    }
   ],
   "source": [
    "# let's get the machinery going\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.cluster import OPTICS, DBSCAN\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import faiss\n",
    "import fast_hdbscan\n",
    "import embcom\n",
    "\n",
    "import lfr\n",
    "import embcom\n",
    "import csv\n",
    "import sys\n",
    "sys.path.append(\"/nobackup/gogandhi/alt_means_sans_k/\")\n",
    "\n",
    "from scripts.nets_and_embeddings import create_and_save_network_and_embedding\n",
    "from scripts.clustering_methods import clustering_method_values\n",
    "\n",
    "from pyclustering.cluster import cluster_visualizer\n",
    "from pyclustering.cluster.xmeans import xmeans, splitting_type\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.utils import read_sample\n",
    "from pyclustering.samples.definitions import FCPS_SAMPLES\n",
    "import numpy as np\n",
    "import belief_propagation\n",
    "import infomap\n",
    "\n",
    "from graph_tool.all import Graph,minimize_blockmodel_dl\n",
    "\n",
    "import graph_tool.all as gt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse, stats\n",
    "from scipy.sparse.csgraph import connected_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6871c394-1224-45c9-a8b1-9aee6cf000c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates element-centric similarity:\n",
    "def calc_esim(y, ypred):\n",
    "\n",
    "    ylab, y = np.unique(y, return_inverse=True)\n",
    "    ypredlab, ypred = np.unique(ypred, return_inverse=True)\n",
    "    \n",
    "    Ka, Kb = len(ylab), len(ypredlab)\n",
    "    K = np.maximum(Ka, Kb)\n",
    "    N = len(y)\n",
    "    \n",
    "    UA = sparse.csr_matrix((np.ones_like(y), (np.arange(y.size), y)), shape=(N,K))\n",
    "    UB = sparse.csr_matrix((np.ones_like(ypred), (np.arange(ypred.size), ypred)), shape=(N, K))    \n",
    "    \n",
    "    nA = np.array(UA.sum(axis=0)).reshape(-1)\n",
    "    nB = np.array(UB.sum(axis=0)).reshape(-1)\n",
    "\n",
    "# nAB[i][j] is read as the number of elements that belong to ith ground truth label and jth predicrted label.\n",
    "# nAB[1][0] = 1 For ground truth label with index 1 and predicted label 0 we have 1 element. i.e. 0000|1| vs 1110|0|\n",
    "\n",
    "    nAB = (UA.T @ UB).toarray()\n",
    "    nAB_rand = np.outer(nA, nB) / N\n",
    "    \n",
    "# assuming that each element has an equal probability of being assigned to any label,\n",
    "# and the expected counts are calculated based on label frequencies.\n",
    "\n",
    "\n",
    "    # Calc element-centric similarity\n",
    "    Q = np.maximum(nA[:, None] @ np.ones((1, K)), np.ones((K, 1)) @ nB[None, :]) \n",
    "    Q = 1 / np.maximum(Q, 1)\n",
    "    S = np.sum(np.multiply(Q, (nAB**2))) / N\n",
    "    \n",
    "    # Calc the expected element-centric similarity for random partitions\n",
    "    #Q = np.maximum(nA[:, None] @ np.ones((1, K)), np.ones((K, 1)) @ nB[None, :]) \n",
    "    #Q = 1 / np.maximum(Q, 1)\n",
    "    Srand = np.sum(np.multiply(Q, (nAB_rand**2))) / N\n",
    "    Scorrected = (S - Srand) / (1 - Srand)\n",
    "    return Scorrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9696213-d8bc-4e0c-ae83-37973ae728af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knn_edges(emb, num_neighbors, \n",
    "                   target=None, metric=\"dotsim\",\n",
    "                   device=None):\n",
    "    \n",
    "    k = int(np.minimum(num_neighbors + 1, emb.shape[0]).astype(int))\n",
    "    indices, distances = find_knn(emb if target is None else target, emb,\n",
    "                                  num_neighbors=k,\n",
    "                                  metric=metric,\n",
    "                                  device=device)\n",
    "    r = np.outer(np.arange(indices.shape[0]), np.ones((1, indices.shape[1]))).astype(int)\n",
    "    r, c, distances = (r.reshape(-1),indices.astype(int).reshape(-1),distances.reshape(-1))\n",
    "    if len(r) == 0:\n",
    "        return r, c, distances \n",
    "    \n",
    "    return r, c, distances\n",
    "\n",
    "# Only place where GPU is used, need to handle its closing. \n",
    "def find_knn(target, emb, num_neighbors, metric=\"dotsim\", device=None): \n",
    "    if metric == \"dotsim\":\n",
    "        index = faiss.IndexFlatIP(emb.shape[1]) \n",
    "    elif metric == \"euclidean\":\n",
    "        index = faiss.IndexFlatL2(emb.shape[1])\n",
    "    elif metric == \"manhattan\":\n",
    "        index = faiss.IndexFlatL1(emb.shape[1])\n",
    "    elif metric == \"cosine\":\n",
    "        index = faiss.IndexFlatIP(emb.shape[1])\n",
    "        emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "    elif metric==\"mahalanobis\":\n",
    "        # THis mathematical trick works, but it needs some reworking in target.astype to\n",
    "        # give right results.\n",
    "        # map the vectors back to a space where they follow a unit Gaussian\n",
    "        xc = emb - emb.mean(0)\n",
    "        cov = np.dot(xc.T, xc) / xc.shape[0]\n",
    "        L = np.linalg.cholesky(cov)\n",
    "        mahalanobis_transform = np.linalg.inv(L)\n",
    "        emb = np.dot(emb, mahalanobis_transform.T)\n",
    "        index = faiss.IndexFlatL2(emb.shape[1])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric specified.\")\n",
    "    \n",
    "    if device is None:\n",
    "        index.add(emb.astype(np.float32))\n",
    "        distances, indices = index.search(target.astype(np.float32), k=num_neighbors)\n",
    "        # This line takes too long to load.\n",
    "    else: \n",
    "        try:\n",
    "            gpu_id = int(device[-1])\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, gpu_id, index)\n",
    "            index.add(emb.astype(np.float32))\n",
    "            distances, indices = index.search(\n",
    "                target.astype(np.float32), k=num_neighbors\n",
    "            )\n",
    "        except RuntimeError:\n",
    "            if metric == \"dotsim\":\n",
    "                index = faiss.IndexFlatIP(emb.shape[1]) \n",
    "            elif metric == \"euclidean\":\n",
    "                index = faiss.IndexFlatL2(emb.shape[1])\n",
    "            elif metric == \"manhattan\":\n",
    "                index = faiss.IndexFlatL1(emb.shape[1])\n",
    "            elif metric == \"cosine\":\n",
    "                index = faiss.IndexFlatIP(emb.shape[1])\n",
    "                emb = emb / np.linalg.norm(emb, axis=1, keepdims=True)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid metric specified.\")\n",
    "            \n",
    "            index.add(emb.astype(np.float32))\n",
    "            distances, indices = index.search(target.astype(np.float32),\n",
    "                                              k=num_neighbors)\n",
    "        \n",
    "    index.reset()\n",
    "    return indices, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b98eeac-ce18-4cdb-a54f-c6ed67b6b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "def louvain(Z, w1, b0, num_neighbors=10, iteration = 50, device = \"cuda:0\", return_member_matrix=False):\n",
    "    \"\"\"\"Louvain algorithm for vecto data\n",
    "    :param Z: embedding\n",
    "    :type Z: numpy.ndarray\n",
    "    :param w1: regression coefficient\n",
    "    :type w1: float\n",
    "    :param b0: intercept\n",
    "    :type b0: float\n",
    "    :param num_neighbors: Number of neighbors, defaults to 100\n",
    "    :type num_neighbors: int, optional\n",
    "    :param iteration: The maximum number of iterations, defaults to 50\n",
    "    :type iteration: int, optional\n",
    "    :param device: _description_, defaults to \"cuda:0\"\n",
    "    :type device: str, optional\n",
    "    :param return_member_matrix: _description_, defaults to False\n",
    "    :type return_member_matrix: bool, optional\n",
    "    :return: _description_\n",
    "    :rtype: _type_\n",
    "    \"\"\"\n",
    "    # Initialize the intermediate variables\n",
    "    num_nodes = Z.shape[0]\n",
    "    node_size = np.ones(num_nodes)\n",
    "    U = sparse.identity(num_nodes, format=\"csr\")\n",
    "    Vt = Z.copy()\n",
    "    \n",
    "    # The main loop for the Louvain algorithm\n",
    "    while True:\n",
    "        # Find the community assignment for the given graph # using a label switching algorithm\n",
    "        cids_t = label_switching(\n",
    "               Z=Vt,\n",
    "               num_neighbors=num_neighbors,\n",
    "               rho=b0/w1,\n",
    "               node_size=node_size,\n",
    "               epochs=iteration,\n",
    "               device=device,\n",
    "                )\n",
    "        \n",
    "        # This is to make the community labels continuous integer variables\n",
    "        _, cids_t = np.unique(cids_t, return_inverse=True)\n",
    "        \n",
    "        # If no merging, we are good to go out from the loop\n",
    "        if int(max(cids_t) + 1) == Vt.shape[0]: \n",
    "            break\n",
    "            \n",
    "        # If two nodes are merged, we created an aggregated network, \n",
    "        #where a node represents a community.\n",
    "        \n",
    "        num_nodes_t = len(cids_t)\n",
    "        k = int(np.max(cids_t) + 1)\n",
    "        Ut = sparse.csr_matrix((np.ones(num_nodes_t), (np.arange(num_nodes_t), cids_t)), shape=(num_nodes_t, k))\n",
    "        U = U @ Ut\n",
    "        Vt = Ut.T @ Vt\n",
    "        \n",
    "        node_size = np.array(Ut.T @ node_size).reshape(-1)\n",
    "    if return_member_matrix: \n",
    "        return U\n",
    "    cids = np.array((U @ sparse.diags(np.arange(U.shape[1]))).sum(axis=1)).reshape(-1)\n",
    "\n",
    "    return cids\n",
    "\n",
    "#\n",
    "# Clustering based on a label switching algorithm\n",
    "#\n",
    "def label_switching(Z, rho, num_neighbors=50, node_size=None, device=None,epochs=50): # This involves distance metrics (cosine similarity, atm)\n",
    "    num_nodes, dim = Z.shape\n",
    "    if node_size is None:\n",
    "        node_size = np.ones(num_nodes)\n",
    "    Z = Z.copy(order=\"C\").astype(np.float32)\n",
    "    # Construct the candidate graph\n",
    "    Z1 = np.hstack([Z, np.ones((num_nodes, 1))])\n",
    "    Zrho = np.hstack([Z, -rho * node_size.reshape((-1, 1))])\n",
    "\n",
    "    r, c, v = find_knn_edges(\n",
    "        Zrho,\n",
    "        target=Z1,\n",
    "        num_neighbors=num_neighbors,\n",
    "        metric=\"cosine\",\n",
    "        device=device)\n",
    "    A = sparse.csr_matrix((v, (r, c)), shape=(num_nodes, num_nodes))\n",
    "    \n",
    "    return _label_switching_(\n",
    "        A_indptr=A.indptr,\n",
    "        A_indices=A.indices,\n",
    "        Z=Z,\n",
    "        num_nodes=num_nodes,\n",
    "        rho=rho,\n",
    "        node_size=node_size,\n",
    "        epochs=epochs)\n",
    "\n",
    "#@numba.jit(nopython=True, cache=True)\n",
    "def _label_switching_(A_indptr, A_indices, Z, num_nodes, rho, node_size,epochs=100):\n",
    "    Nc = np.zeros(num_nodes)\n",
    "    cids = np.arange(num_nodes)\n",
    "    Vc = Z.copy()\n",
    "    Vnorm = np.sum(np.multiply(Z, Z), axis=1).reshape(-1) \n",
    "    for nid in range(num_nodes):\n",
    "            Nc[nid] += node_size[nid]\n",
    "    for _it in range(epochs):\n",
    "        order = np.random.choice(num_nodes, size=num_nodes, replace=False) \n",
    "        updated_node_num = 0\n",
    "        \n",
    "        for _k, node_id in enumerate(order):\n",
    "            # Get the weight and normalized weight\n",
    "            neighbors = A_indices[A_indptr[node_id] : A_indptr[node_id + 1]]\n",
    "\n",
    "            # Calculate the grain\n",
    "            c = cids[node_id]\n",
    "            clist = np.unique(cids[neighbors])\n",
    "            next_cid = -1\n",
    "            dqmax = 0\n",
    "            qself = (\n",
    "                np.sum(Z[node_id, :] * Vc[c, :])\n",
    "                - Vnorm[node_id]\n",
    "                - rho * node_size[node_id] * (Nc[c] - node_size[node_id]))\n",
    "\n",
    "            for cprime in clist:\n",
    "                if c == cprime: \n",
    "                    continue\n",
    "                dq = (np.sum(Z[node_id, :] * Vc[cprime, :])\n",
    "                        - rho * node_size[node_id] * Nc[cprime]) - qself\n",
    "                if dqmax < dq:\n",
    "                    next_cid = cprime\n",
    "                    dqmax = dq\n",
    "            if dqmax <= 1e-16: \n",
    "                continue\n",
    "\n",
    "            Nc[c] -= node_size[node_id]\n",
    "            Nc[next_cid] += node_size[node_id]\n",
    "\n",
    "            Vc[c, :] -= Z[node_id, :]\n",
    "            Vc[next_cid, :] += Z[node_id, :]\n",
    "\n",
    "            cids[node_id] = next_cid\n",
    "            updated_node_num += 1\n",
    "\n",
    "        if (updated_node_num / np.maximum(1, num_nodes)) < 1e-3: \n",
    "            break\n",
    "    return cids\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544ef12b-15d8-4e8b-991c-fe635294f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_method_values(net, community_table, emb, score_keys, device_name):\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize the vector of each node to have unit length. This normalization improves clustering.\n",
    "    X = np.einsum(\"ij,i->ij\", emb, 1 / np.maximum(np.linalg.norm(emb, axis=1), 1e-24))\n",
    "    X = emb.copy()\n",
    "    # Clustering\n",
    "\n",
    "    def proposed_method_labels(emb,device_name):\n",
    "        rpos, cpos, vpos = find_knn_edges(emb, num_neighbors=500, device = device_name)\n",
    "        cneg = np.random.choice(emb.shape[0], len(cpos))\n",
    "        vneg = np.array(np.sum(emb[rpos, :] * emb[cneg, :], axis=1)).reshape(-1)\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(\n",
    "            np.concatenate([vpos, vneg]).reshape((-1, 1)),\n",
    "            np.concatenate([np.ones_like(vpos), np.zeros_like(vneg)]),\n",
    "                )\n",
    "        w1, b0 = model.coef_[0, 0], -model.intercept_[0] \n",
    "        return louvain(emb, w1, b0, device = device_name)\n",
    "\n",
    "    # Evaluate the clustering\n",
    "    def method_score(key):\n",
    "        if key == \"kmeans\": # Does use k\n",
    "            kmeans = KMeans(n_clusters= len(set(community_table[\"community_id\"])), random_state=0).fit(X)\n",
    "            return calc_esim(community_table[\"community_id\"], kmeans.labels_)\n",
    "        \n",
    "        if key == \"dbscan\": # Does kinda use k (kinda we give k as minimum cluster size to avoid errors)\n",
    "            \n",
    "            clusterer = fast_hdbscan.HDBSCAN(min_cluster_size=len(set(community_table[\"community_id\"])))\n",
    "            dbscan_labels = clusterer.fit_predict(X)\n",
    "            return calc_esim(community_table[\"community_id\"], dbscan_labels)\n",
    "        \n",
    "        if key == \"optics\": # Does not use k\n",
    "            optics = OPTICS().fit(X)\n",
    "            return calc_esim(community_table[\"community_id\"], optics.labels_)\n",
    "        \n",
    "        if key == \"proposed\": # Does not use k\n",
    "            return calc_esim(community_table[\"community_id\"], proposed_method_labels(emb,device_name)) \n",
    "        \n",
    "        if key == \"xmeans\": # Does use k\n",
    "            # Create instance of X-Means algorithm with MNDL splitting criterion.\n",
    "            initial_centers = kmeans_plusplus_initializer(X, amount_centers=len(set(community_table['community_id']))).initialize()\n",
    "            xmeans_mndl = xmeans(X, initial_centers, 20, splitting_type=splitting_type.MINIMUM_NOISELESS_DESCRIPTION_LENGTH)\n",
    "            xmeans_mndl.process()\n",
    "            mndl_clusters = xmeans_mndl.get_clusters()\n",
    "            xmeans_labels = [i[1] for i in sorted([(j,i) for i in range(len(mndl_clusters)) for j in mndl_clusters[i]])]\n",
    "\n",
    "            return calc_esim(community_table[\"community_id\"], xmeans_labels)\n",
    "        \n",
    "        if key == \"belief_prop\": # Does use k\n",
    "            belief_prop_labels = belief_propagation.detect(net, q=len(set(community_table['community_id'])), init_memberships=community_table[\"community_id\"]) \n",
    "            return calc_esim(community_table[\"community_id\"], belief_prop_labels)\n",
    "        \n",
    "        if key == \"infomap\": # Does not use k\n",
    "            r, c, v = sparse.find(net + net.T)\n",
    "            im = infomap.Infomap(silent=True)\n",
    "            for i in range(len(r)):\n",
    "                im.add_link(r[i], c[i], 1)\n",
    "            im.run()\n",
    "            \n",
    "            cids = np.zeros(net.shape[0])\n",
    "            for node in im.tree:\n",
    "                if node.is_leaf:\n",
    "                    cids[node.node_id] = node.module_id\n",
    "                    \n",
    "            infomap_labels = np.unique(cids, return_inverse=True)[1]\n",
    "\n",
    "            return calc_esim(community_table[\"community_id\"], infomap_labels)\n",
    "            \n",
    "        if key == \"flatsbm\": # Does use k\n",
    "            r, c, v = sparse.find(net)\n",
    "            g = Graph(directed=False)\n",
    "            g.add_edge_list(np.vstack([r, c]).T)\n",
    "            K = len(set(community_table['community_id']))\n",
    "            state = minimize_blockmodel_dl(\n",
    "                g,\n",
    "                state_args={\"B_min\": K, \"B_max\": K},\n",
    "                multilevel_mcmc_args={\"B_max\": K, \"B_min\": K},\n",
    "            )\n",
    "            b = state.get_blocks()\n",
    "            flatsbm_labels = np.unique(np.array(b.a), return_inverse=True)[1]\n",
    "            return calc_esim(community_table[\"community_id\"], flatsbm_labels)\n",
    "            \n",
    "    score_dictionary={}\n",
    "    for key in score_keys:\n",
    "        score_dictionary[key] = method_score(key)\n",
    "    \n",
    "    return score_dictionary\n",
    "\n",
    "def get_scores(params= None, emb_params = None, score_keys = None, path_name = None, device_name = \"cuda:0\"):\n",
    "    \n",
    "    \n",
    "    # Defaults\n",
    "    if params is None:\n",
    "        params = {\n",
    "            \"N\": 100000,     # number of nodes\n",
    "            \"k\": 50,       # average degree\n",
    "            \"maxk\": 1000,   # maximum degree sqrt(10*N)\n",
    "            \"minc\": 50,    # minimum community size\n",
    "            \"maxc\": 1000,   # maximum community size sqrt(10*N)\n",
    "            \"tau\": 3.0,    # degree exponent\n",
    "            \"tau2\": 1.2,   # community size exponent\n",
    "            \"mu\": 0.2,     # mixing rate\n",
    "        }\n",
    "    if emb_params is None:\n",
    "        emb_params = {      \"method\": \"node2vec\",\n",
    "                            \"window_length\": 10,\n",
    "                            \"walk_length\": 80,\n",
    "                            \"num_walks\": 10,\n",
    "                            \"dim\" : 64,\n",
    "                            }\n",
    "    \n",
    "    if score_keys is None:\n",
    "        score_keys = ['kmeans', 'dbscan', 'optics', 'proposed','xmeans','belief_prop','infomap','flatsbm']\n",
    "    \n",
    "    # Allowing existing files in path to be uses. We might need to generate all nets and embeddings first and then proceed to clustering.\n",
    "    # Easier for snakemake as well.\n",
    "\n",
    "    net, community_table, emb = create_and_save_network_and_embedding(params,emb_params, path_name, save_file=True)\n",
    "\n",
    "    return clustering_method_values(net, community_table, emb, score_keys,device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b7c1ebf-5b14-49c0-90dc-7861c1bafa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt_means_ootb(emb, communities, device_name=\"cuda:0\", num_neighbors=500): \n",
    "    # Here emb is just an array of datapoints, where each row corresponds to a d dimensional array corresponding to a point.\n",
    "    # Community table is an array with community partitions, which will be converted \n",
    "    # below into a pandas dataframe, with 'community_id' column containing the array data.\n",
    "\n",
    "    \n",
    "\n",
    "    # Normalize the vector of each node to have unit length. This normalization improves clustering.\n",
    "    X = np.einsum(\"ij,i->ij\", emb, 1 / np.maximum(np.linalg.norm(emb, axis=1), 1e-24))\n",
    "    X = emb.copy()\n",
    "    # Clustering\n",
    "\n",
    "\n",
    "    rpos, cpos, vpos = find_knn_edges(emb, num_neighbors=num_neighbors, device = device_name)\n",
    "    cneg = np.random.choice(emb.shape[0], len(cpos))\n",
    "    vneg = np.array(np.sum(emb[rpos, :] * emb[cneg, :], axis=1)).reshape(-1)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(\n",
    "        np.concatenate([vpos, vneg]).reshape((-1, 1)),\n",
    "        np.concatenate([np.ones_like(vpos), np.zeros_like(vneg)]),\n",
    "            )\n",
    "    w1, b0 = model.coef_[0, 0], -model.intercept_[0] \n",
    "\n",
    "    kmeans = KMeans(n_clusters= len(set(community_table[\"community_id\"])), random_state=0).fit(X)\n",
    "    kmeans_esim = calc_esim(communities, kmeans.labels_)\n",
    "        \n",
    "    \n",
    "    proposed_labels = louvain(emb, w1, b0, device = device_name)\n",
    "    esim = calc_esim(communities, proposed_labels) \n",
    "        \n",
    "    \n",
    "    return proposed_labels, esim #, kmeans_labels, kmeans_esim \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93399ff5-6875-4b8e-a515-0edaabc71946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmeans_env",
   "language": "python",
   "name": "kmeans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
