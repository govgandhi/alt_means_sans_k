{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9a8b3a-712d-44c4-824f-abf80ece7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import csv\n",
    "from itertools import cycle\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# warnings.resetwarnings() # To change it back (optional)\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/nobackup/gogandhi/alt_means_sans_k/\")\n",
    "\n",
    "from scripts.similarity_scores import clustering_method_values, get_scores\n",
    "from scripts.nets_and_embeddings import *\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/nobackup/gogandhi/alt_means_sans_k/notebooks/20240513_changing_binary_classifier/Logistic-Regression-From-Scratch-main/src\")\n",
    "from custom_logistic_regression import LogisticRegression as CustomLogisticRegression\n",
    "\n",
    "from scripts.similarity_scores import louvain, calc_esim,find_knn_edges\n",
    "\n",
    "\n",
    "\n",
    "def alt_means_ootb(emb, communities, device_name=\"cuda:0\", num_neighbors=100): \n",
    "    # Here emb is just an array of datapoints, where each row corresponds to a d dimensional array corresponding to a point.\n",
    "    # Community table is an array with community partitions, which will be converted \n",
    "    # below into a pandas dataframe, with 'community_id' column containing the array data.\n",
    "\n",
    "    \n",
    "\n",
    "    # Normalize the vector of each node to have unit length. This normalization improves clustering.\n",
    "    X = np.einsum(\"ij,i->ij\", emb, 1 / np.maximum(np.linalg.norm(emb, axis=1), 1e-24)) \n",
    "    #This makes our binary classifier from dot to cosine similarity\n",
    "    X = emb.copy()\n",
    "\n",
    "    # Clustering\n",
    "\n",
    "\n",
    "    rpos, cpos, vpos = find_knn_edges(emb, num_neighbors=num_neighbors, device = device_name)\n",
    "    cneg = np.random.choice(emb.shape[0], len(cpos))\n",
    "    vneg = np.array(np.sum(emb[rpos, :] * emb[cneg, :], axis=1)).reshape(-1)\n",
    "    \n",
    "    x_input = np.concatenate([vpos, vneg]).reshape((-1, 1))\n",
    "    y_predicted = np.concatenate([np.ones_like(vpos), np.zeros_like(vneg)])\n",
    "    model = LogisticRegression()\n",
    "    model.fit(\n",
    "        x_input,\n",
    "        y_predicted),\n",
    "            \n",
    "    w1, b0 = model.coef_[0, 0], -model.intercept_[0] \n",
    "\n",
    "    ##############\n",
    "    bias_coef = np.linalg.norm(np.sum(emb,axis=0)/N)\n",
    "    \n",
    "    lr = CustomLogisticRegression()\n",
    "    lr.fit(x_input, y_predicted, bias_coef, epochs=150)\n",
    "    pred = lr.predict(x_test)\n",
    "    weights, bias = lr.coef_()\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    print(accuracy)\n",
    "    ##############\n",
    "    \n",
    "    kmeans = KMeans(n_clusters= len(set(communities)), random_state=0).fit(X)\n",
    "    kmeans_esim = calc_esim(communities, kmeans.labels_)\n",
    "    \n",
    "    \n",
    "    proposed_labels = louvain(emb, w1, b0, device = device_name)\n",
    "    esim = calc_esim(communities, proposed_labels) \n",
    "        \n",
    "    \n",
    "    return proposed_labels, kmeans.labels_, esim, kmeans_esim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7b9fce-0976-4116-b1c3-fe926b380d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get network and emb data to try on the custom model\n",
    "\n",
    "\n",
    "N=10000\n",
    "params = {\n",
    "    \"N\": N,\n",
    "    \"k\": 50,\n",
    "    \"maxk\":  int(np.sqrt(10 * N)),\n",
    "    \"minc\": 50,\n",
    "    \"maxc\": int(np.ceil(np.sqrt(N * 10))),\n",
    "    \"tau\": 3.0,\n",
    "    \"tau2\": 1.0,\n",
    "    \"mu\": 0.65,\n",
    "    }\n",
    "            \n",
    "emb_params = {\n",
    "    \"method\": \"node2vec\",\n",
    "    \"window_length\": 10,\n",
    "    \"walk_length\": 80,\n",
    "    \"num_walks\": 10,\n",
    "    \"dim\": 64,\n",
    "}\n",
    "\n",
    "net, community_table, seed = create_network(params)\n",
    "emb = create_embedding(net, emb_params)\n",
    "\n",
    "#result_run_mu = clustering_method_values(net, community_table, emb, score_keys, \"cuda:3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e2578-e1d7-44ff-9a8e-6e50e516c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data import x_train, x_test, y_train, y_test\n",
    "\n",
    "# bias_coef = np.linalg.norm(np.sum(x,axis=0)/N)\n",
    "\n",
    "# lr = CustomLogisticRegression()\n",
    "# lr.fit(x_train, y_train, bias_coef, epochs=150)\n",
    "# pred = lr.predict(x_test)\n",
    "# weights, bias = lr.coef_()\n",
    "# accuracy = accuracy_score(y_test, pred)\n",
    "# print(accuracy)\n",
    "\n",
    "# model = LogisticRegression(solver='newton-cg', max_iter=150)\n",
    "# model.fit(x_train, y_train)\n",
    "# pred2 = model.predict(x_test)\n",
    "# accuracy2 = accuracy_score(y_test, pred2)\n",
    "# print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b818c-72ca-4897-bfa8-3264de460ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For restarting kernel\n",
    "\n",
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2264f5-36a9-4e13-b955-aa1a32a41c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03e8c1e4-86d2-4865-a324-b2b6462caf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the vector of each node to have unit length. This normalization improves clustering.\n",
    "X = np.einsum(\"ij,i->ij\", emb, 1 / np.maximum(np.linalg.norm(emb, axis=1), 1e-24)) \n",
    "#This makes our binary classifier from dot to cosine similarity\n",
    "X = emb.copy()\n",
    "\n",
    "# Clustering\n",
    "\n",
    "rpos, cpos, vpos = find_knn_edges(emb, num_neighbors=100, device = \"cuda:0\")\n",
    "cneg = np.random.choice(emb.shape[0], len(cpos))\n",
    "vneg = np.array(np.sum(emb[rpos, :] * emb[cneg, :], axis=1)).reshape(-1)\n",
    "\n",
    "bias_coef = np.linalg.norm(np.sum(emb,axis=0)/N)\n",
    "\n",
    "x_input = np.concatenate([vpos, vneg]).reshape((-1, 1)) - bias_coef\n",
    "y_predicted = np.concatenate([np.ones_like(vpos), np.zeros_like(vneg)])\n",
    "\n",
    "                             \n",
    "model = LogisticRegression(fit_intercept=True)\n",
    "model.fit(\n",
    "    x_input,\n",
    "    y_predicted)\n",
    "w0, b0 = model.coef_[0, 0], -model.intercept_[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0a331e0-83e1-438a-b043-8f3b3e473794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1373434635269928, -0.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0,b0 # OG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "332f1643-1669-4476-9005-008f8e90d7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.120677304859043, 25.9372287597817)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1,b1 # added bias with intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc7e4b-9da0-4458-a225-b4c9ee7d93a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.097692992403809, 8.875957487682873)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3612323d-3ac7-44c2-bbae-d1eb574f758c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14020269574223815, -0.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2,b2 # added bias without intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468cb5e0-25e4-4f37-b35a-8f20f82bbbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1367683364344097, -0.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4243602d-8784-4c5a-8f3b-bbd9acbdcd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "proposed_labels = louvain(emb, w0, b0, device = \"cuda:0\")\n",
    "esim = calc_esim(community_table['community_id'], proposed_labels) \n",
    "print(esim)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f7f9cb4-db34-449a-b0de-375a7458aaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2066986484057361\n"
     ]
    }
   ],
   "source": [
    "proposed_labels = louvain(emb, w1, b1, device = \"cuda:0\")\n",
    "esim = calc_esim(community_table['community_id'], proposed_labels) \n",
    "print(esim)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0da4ca49-bd70-4980-9472-21514b2a667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "proposed_labels = louvain(emb, w2, b2, device = \"cuda:0\")\n",
    "esim = calc_esim(community_table['community_id'], proposed_labels) \n",
    "print(esim)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0738c7e-73f7-49b4-8e4b-9b1f5371fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# import sys\n",
    "# sys.path.append(\"/nobackup/gogandhi/alt_means_sans_k/notebooks/20240513_changing_binary_classifier/Logistic-Regression-From-Scratch-main/src\")\n",
    "# from custom_logistic_regression import LogisticRegression as CustomLogisticRegression\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class CustomLogisticRegression():\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.train_accuracies = []\n",
    "\n",
    "\n",
    "    # We are modifying the function as:\n",
    "    #    y = (1 + exp(w.x + x_c)))^-1\n",
    "    def fit(self, x, y, bias_coef, epochs):\n",
    "        #x = self._transform_x(x)\n",
    "        #y = self._transform_y(y)\n",
    "\n",
    "        self.weights = np.zeros(x.shape[1])\n",
    "        self.bias = bias_coef\n",
    "\n",
    "        for i in range(epochs):\n",
    "            x_dot_weights = np.matmul(self.weights, x.transpose()) + self.bias\n",
    "            pred = self._sigmoid(x_dot_weights)\n",
    "            loss = self.compute_loss(y, pred)\n",
    "            error_w, error_b = self.compute_gradients(x, y, pred)\n",
    "            self.update_model_parameters(error_w, error_b)\n",
    "\n",
    "            pred_to_class = [1 if p > 0.5 else 0 for p in pred]\n",
    "            self.train_accuracies.append(accuracy_score(y, pred_to_class))\n",
    "            self.losses.append(loss)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # binary cross entropy\n",
    "        y_zero_loss = y_true * np.log(y_pred + 1e-9)\n",
    "        y_one_loss = (1-y_true) * np.log(1 - y_pred + 1e-9)\n",
    "        return -np.mean(y_zero_loss + y_one_loss)\n",
    "\n",
    "    def compute_gradients(self, x, y_true, y_pred):\n",
    "        # derivative of binary cross entropy\n",
    "        difference =  y_pred - y_true\n",
    "        gradient_b = np.mean(difference)\n",
    "        gradients_w = np.matmul(x.transpose(), difference)\n",
    "        gradients_w = np.array([np.mean(grad) for grad in gradients_w])\n",
    "\n",
    "        return gradients_w, gradient_b\n",
    "\n",
    "    def update_model_parameters(self, error_w, error_b):\n",
    "        self.weights = self.weights - 0.1 * error_w\n",
    "        #self.bias = self.bias - 0.1 * error_b \n",
    "        # Our bias is a fixed average similarity of vectors, so we don't update it\n",
    "\n",
    "    def predict(self, x):\n",
    "        x_dot_weights = np.matmul(x, self.weights.transpose()) + self.bias\n",
    "        probabilities = self._sigmoid(x_dot_weights)\n",
    "        return [1 if p > 0.5 else 0 for p in probabilities]\n",
    "    \n",
    "    def coef_(self):\n",
    "        return self.weights, self.bias\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return np.array([self._sigmoid_function(value) for value in x])\n",
    "\n",
    "    def _sigmoid_function(self, x):\n",
    "        if x >= 0:\n",
    "            z = np.exp(-x)\n",
    "            return 1 / (1 + z)\n",
    "        else:\n",
    "            z = np.exp(x)\n",
    "            return z / (1 + z)\n",
    "\n",
    "    def _transform_x(self, x):\n",
    "        x = copy.deepcopy(x)\n",
    "        return x.values\n",
    "\n",
    "    def _transform_y(self, y):\n",
    "        y = copy.deepcopy(y)\n",
    "        return y.values.reshape(y.shape[0], 1)\n",
    "\n",
    "\n",
    "bias_coef = np.linalg.norm(np.sum(emb,axis=0)/N)\n",
    "\n",
    "lr = CustomLogisticRegression()\n",
    "lr.fit(x_input, y_predicted, bias_coef, epochs=10)\n",
    "weights, bias = lr.coef_()\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04525e9e-15e2-4dce-b4fe-3ca594ffc3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([102377.68488969]), 1.1957085215886312)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ab7576-1d10-49a9-b26e-ec463595366c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (2020000, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import x_train, x_test, y_train, y_test\n",
    "\n",
    "np.shape(x_train),np.shape(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3565478-7834-428c-bdfe-28afd00e1fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>9.029</td>\n",
       "      <td>17.33</td>\n",
       "      <td>58.79</td>\n",
       "      <td>250.5</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.31300</td>\n",
       "      <td>0.04375</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.08046</td>\n",
       "      <td>...</td>\n",
       "      <td>10.310</td>\n",
       "      <td>22.65</td>\n",
       "      <td>65.50</td>\n",
       "      <td>324.7</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.43650</td>\n",
       "      <td>1.25200</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.11750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>21.090</td>\n",
       "      <td>26.57</td>\n",
       "      <td>142.70</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.24870</td>\n",
       "      <td>0.14960</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.07398</td>\n",
       "      <td>...</td>\n",
       "      <td>26.680</td>\n",
       "      <td>33.48</td>\n",
       "      <td>176.50</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.75840</td>\n",
       "      <td>0.67800</td>\n",
       "      <td>0.29030</td>\n",
       "      <td>0.4098</td>\n",
       "      <td>0.12840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>9.173</td>\n",
       "      <td>13.86</td>\n",
       "      <td>59.20</td>\n",
       "      <td>260.9</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.05988</td>\n",
       "      <td>0.02180</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.06963</td>\n",
       "      <td>...</td>\n",
       "      <td>10.010</td>\n",
       "      <td>19.23</td>\n",
       "      <td>65.59</td>\n",
       "      <td>310.1</td>\n",
       "      <td>0.09836</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.13970</td>\n",
       "      <td>0.05087</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.08490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>10.650</td>\n",
       "      <td>25.22</td>\n",
       "      <td>68.01</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.07234</td>\n",
       "      <td>0.02379</td>\n",
       "      <td>0.01615</td>\n",
       "      <td>0.1897</td>\n",
       "      <td>0.06329</td>\n",
       "      <td>...</td>\n",
       "      <td>12.250</td>\n",
       "      <td>35.19</td>\n",
       "      <td>77.98</td>\n",
       "      <td>455.7</td>\n",
       "      <td>0.14990</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.06136</td>\n",
       "      <td>0.3409</td>\n",
       "      <td>0.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10.170</td>\n",
       "      <td>14.88</td>\n",
       "      <td>64.55</td>\n",
       "      <td>311.9</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.08061</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.01290</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.06960</td>\n",
       "      <td>...</td>\n",
       "      <td>11.020</td>\n",
       "      <td>17.45</td>\n",
       "      <td>69.86</td>\n",
       "      <td>368.6</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.02168</td>\n",
       "      <td>0.02579</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>0.08020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>8.888</td>\n",
       "      <td>14.64</td>\n",
       "      <td>58.79</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.15310</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.02872</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.08980</td>\n",
       "      <td>...</td>\n",
       "      <td>9.733</td>\n",
       "      <td>15.67</td>\n",
       "      <td>62.56</td>\n",
       "      <td>284.4</td>\n",
       "      <td>0.12070</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.14340</td>\n",
       "      <td>0.04786</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>11.640</td>\n",
       "      <td>18.33</td>\n",
       "      <td>75.17</td>\n",
       "      <td>412.5</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.07070</td>\n",
       "      <td>0.03485</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.06520</td>\n",
       "      <td>...</td>\n",
       "      <td>13.140</td>\n",
       "      <td>29.26</td>\n",
       "      <td>85.51</td>\n",
       "      <td>521.7</td>\n",
       "      <td>0.16880</td>\n",
       "      <td>0.26600</td>\n",
       "      <td>0.28730</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2806</td>\n",
       "      <td>0.09097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>14.290</td>\n",
       "      <td>16.82</td>\n",
       "      <td>90.30</td>\n",
       "      <td>632.6</td>\n",
       "      <td>0.06429</td>\n",
       "      <td>0.02675</td>\n",
       "      <td>0.00725</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.05376</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>20.65</td>\n",
       "      <td>94.44</td>\n",
       "      <td>684.6</td>\n",
       "      <td>0.08567</td>\n",
       "      <td>0.05036</td>\n",
       "      <td>0.03866</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>13.980</td>\n",
       "      <td>19.62</td>\n",
       "      <td>91.12</td>\n",
       "      <td>599.5</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.06463</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06544</td>\n",
       "      <td>...</td>\n",
       "      <td>17.040</td>\n",
       "      <td>30.80</td>\n",
       "      <td>113.90</td>\n",
       "      <td>869.3</td>\n",
       "      <td>0.16130</td>\n",
       "      <td>0.35680</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>12.180</td>\n",
       "      <td>20.52</td>\n",
       "      <td>77.22</td>\n",
       "      <td>458.7</td>\n",
       "      <td>0.08013</td>\n",
       "      <td>0.04038</td>\n",
       "      <td>0.02383</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.05677</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340</td>\n",
       "      <td>32.84</td>\n",
       "      <td>84.58</td>\n",
       "      <td>547.8</td>\n",
       "      <td>0.11230</td>\n",
       "      <td>0.08862</td>\n",
       "      <td>0.11450</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.06878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "68         9.029         17.33           58.79      250.5          0.10660   \n",
       "181       21.090         26.57          142.70     1311.0          0.11410   \n",
       "63         9.173         13.86           59.20      260.9          0.07721   \n",
       "248       10.650         25.22           68.01      347.0          0.09657   \n",
       "60        10.170         14.88           64.55      311.9          0.11340   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "71         8.888         14.64           58.79      244.0          0.09783   \n",
       "106       11.640         18.33           75.17      412.5          0.11420   \n",
       "270       14.290         16.82           90.30      632.6          0.06429   \n",
       "435       13.980         19.62           91.12      599.5          0.10600   \n",
       "102       12.180         20.52           77.22      458.7          0.08013   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "68            0.14130         0.31300              0.04375         0.2111   \n",
       "181           0.28320         0.24870              0.14960         0.2395   \n",
       "63            0.08751         0.05988              0.02180         0.2341   \n",
       "248           0.07234         0.02379              0.01615         0.1897   \n",
       "60            0.08061         0.01084              0.01290         0.2743   \n",
       "..                ...             ...                  ...            ...   \n",
       "71            0.15310         0.08606              0.02872         0.1902   \n",
       "106           0.10170         0.07070              0.03485         0.1801   \n",
       "270           0.02675         0.00725              0.00625         0.1508   \n",
       "435           0.11330         0.11260              0.06463         0.1669   \n",
       "102           0.04038         0.02383              0.01770         0.1739   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "68                  0.08046  ...        10.310          22.65   \n",
       "181                 0.07398  ...        26.680          33.48   \n",
       "63                  0.06963  ...        10.010          19.23   \n",
       "248                 0.06329  ...        12.250          35.19   \n",
       "60                  0.06960  ...        11.020          17.45   \n",
       "..                      ...  ...           ...            ...   \n",
       "71                  0.08980  ...         9.733          15.67   \n",
       "106                 0.06520  ...        13.140          29.26   \n",
       "270                 0.05376  ...        14.910          20.65   \n",
       "435                 0.06544  ...        17.040          30.80   \n",
       "102                 0.05677  ...        13.340          32.84   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "68             65.50       324.7           0.14820            0.43650   \n",
       "181           176.50      2089.0           0.14910            0.75840   \n",
       "63             65.59       310.1           0.09836            0.16780   \n",
       "248            77.98       455.7           0.14990            0.13980   \n",
       "60             69.86       368.6           0.12750            0.09866   \n",
       "..               ...         ...               ...                ...   \n",
       "71             62.56       284.4           0.12070            0.24360   \n",
       "106            85.51       521.7           0.16880            0.26600   \n",
       "270            94.44       684.6           0.08567            0.05036   \n",
       "435           113.90       869.3           0.16130            0.35680   \n",
       "102            84.58       547.8           0.11230            0.08862   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "68           1.25200               0.17500          0.4228   \n",
       "181          0.67800               0.29030          0.4098   \n",
       "63           0.13970               0.05087          0.3282   \n",
       "248          0.11250               0.06136          0.3409   \n",
       "60           0.02168               0.02579          0.3557   \n",
       "..               ...                   ...             ...   \n",
       "71           0.14340               0.04786          0.2254   \n",
       "106          0.28730               0.12180          0.2806   \n",
       "270          0.03866               0.03333          0.2458   \n",
       "435          0.40690               0.18270          0.3179   \n",
       "102          0.11450               0.07431          0.2694   \n",
       "\n",
       "     worst fractal dimension  \n",
       "68                   0.11750  \n",
       "181                  0.12840  \n",
       "63                   0.08490  \n",
       "248                  0.08147  \n",
       "60                   0.08020  \n",
       "..                       ...  \n",
       "71                   0.10840  \n",
       "106                  0.09097  \n",
       "270                  0.06120  \n",
       "435                  0.10550  \n",
       "102                  0.06878  \n",
       "\n",
       "[455 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7afae4-f1af-49cd-b965-bd3bd7b360e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b116f-00f0-4cfb-81b4-adf5467cdf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmeans_env",
   "language": "python",
   "name": "kmeans_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
