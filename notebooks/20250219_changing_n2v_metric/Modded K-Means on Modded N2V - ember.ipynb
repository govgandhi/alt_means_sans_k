{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294b892e-2a06-4ea4-b028-b7f719235af9",
   "metadata": {},
   "source": [
    "We have obtained the embeddings for 10,000 node networks of params in this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9983eab2-7aeb-40d3-a71e-561487e14241",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5 # k = {5,10,50}\n",
    "mu = 0.1\n",
    "run_no = 1\n",
    "\n",
    "base = f\"/nobackup/gogandhi/alt_means_sans_k/data/experiment_n2v_metric_change_10000_{k}_3.0_minc50/Run_{run_no}/\" \n",
    "\n",
    "net_filename = f\"net_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{k}_mincomm_50.npz\"  # A = sp.load_npz(net_path)\n",
    "comm_filename = f\"community_table_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{k}_mincomm_50.csv\" # pd.read_csv()\n",
    "emb_filename = f\"embeddings_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{k}_mincomm_50.pkl\" # embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d247f-de93-4272-9ac2-7349337034ec",
   "metadata": {},
   "source": [
    "For instance, we load it to see the embeddings as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c02f866-885a-4eb0-b546-9c80e2b64546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dot', 'euclidean', 'cosine'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(base+emb_filename, 'rb') as f:  # open a text file\n",
    "    emb_dict = pickle.load(f) # deserialize using load()\n",
    "emb_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa962c-4932-4a0d-8345-b7dec59c29cf",
   "metadata": {},
   "source": [
    "Now we want to take these embeddings, and run k-means clustering with different metrics on them to see which combination comes out on top.\n",
    "N2V ... K-Means \\\n",
    "Euc ... Dot? \\\n",
    "Euc ... Euc? \\\n",
    "Dot ... Dot? \\\n",
    "I have a strong feeling this is bound to change based on the dimensionality of the embedding vectors, so I will test the cases with embedding dimensions = 8,16,32,128 also. But we're getting ahead of ourselves now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0f4e-0769-4d80-8416-3e4848ef99f3",
   "metadata": {},
   "source": [
    "# The modified K-Means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "543c2afb-7990-4afe-a017-aad5857d6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates element-centric similarity:\n",
    "def calc_esim(y, ypred):\n",
    "\n",
    "    ylab, y = np.unique(y, return_inverse=True)\n",
    "    ypredlab, ypred = np.unique(ypred, return_inverse=True)\n",
    "    \n",
    "    Ka, Kb = len(ylab), len(ypredlab)\n",
    "    K = np.maximum(Ka, Kb)\n",
    "    N = len(y)\n",
    "    \n",
    "    UA = sparse.csr_matrix((np.ones_like(y), (np.arange(y.size), y)), shape=(N,K))\n",
    "    UB = sparse.csr_matrix((np.ones_like(ypred), (np.arange(ypred.size), ypred)), shape=(N, K))    \n",
    "    \n",
    "    nA = np.array(UA.sum(axis=0)).reshape(-1)\n",
    "    nB = np.array(UB.sum(axis=0)).reshape(-1)\n",
    "\n",
    "# nAB[i][j] is read as the number of elements that belong to ith ground truth label and jth predicrted label.\n",
    "# nAB[1][0] = 1 For ground truth label with index 1 and predicted label 0 we have 1 element. i.e. 0000|1| vs 1110|0|\n",
    "\n",
    "    nAB = (UA.T @ UB).toarray()\n",
    "    nAB_rand = np.outer(nA, nB) / N\n",
    "    \n",
    "# assuming that each element has an equal probability of being assigned to any label,\n",
    "# and the expected counts are calculated based on label frequencies.\n",
    "\n",
    "\n",
    "    # Calc element-centric similarity\n",
    "    Q = np.maximum(nA[:, None] @ np.ones((1, K)), np.ones((K, 1)) @ nB[None, :]) \n",
    "    Q = 1 / np.maximum(Q, 1)\n",
    "    S = np.sum(np.multiply(Q, (nAB**2))) / N\n",
    "    \n",
    "    # Calc the expected element-centric similarity for random partitions\n",
    "    #Q = np.maximum(nA[:, None] @ np.ones((1, K)), np.ones((K, 1)) @ nB[None, :]) \n",
    "    #Q = 1 / np.maximum(Q, 1)\n",
    "    Srand = np.sum(np.multiply(Q, (nAB_rand**2))) / N\n",
    "    Scorrected = (S - Srand) / (1 - Srand)\n",
    "    return Scorrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54a81f2e-9de9-4694-a928-6ea93dfbf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils import check_random_state\n",
    "from numba import njit\n",
    "from sklearn.cluster import KMeans \n",
    "from scipy import sparse\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "class CustomKMeans:\n",
    "    def __init__(self, n_clusters, metric='euclidean', max_iter=300, tol=1e-4, random_state=None, n_init=20, init='k-means++', batch_size=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.metric = metric\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.n_init = n_init\n",
    "        self.init = init\n",
    "        self.batch_size = batch_size  # Mini-batch size, if any\n",
    "\n",
    "    def calculate_distances(self, X, centroids):\n",
    "        \"\"\"Optimized distance calculation for the specified metric.\"\"\"\n",
    "        if self.metric == 'euclidean':\n",
    "            return cdist(X, centroids, metric='euclidean')\n",
    "        elif self.metric == 'manhattan':\n",
    "            return cdist(X, centroids, metric='cityblock')\n",
    "        elif self.metric == 'cosine':\n",
    "            return 1 - cosine_similarity(X, centroids)\n",
    "        elif self.metric == 'dot':\n",
    "            return -np.dot(X, centroids.T)\n",
    "        elif self.metric == 'geodesic':\n",
    "            # Calculate geodesic distance as arccos(cosine_similarity) for normalized data\n",
    "            cos_sim = cosine_similarity(X, centroids)\n",
    "            # Clip values to avoid out-of-domain errors in arccos\n",
    "            cos_sim = np.clip(cos_sim, -1.0, 1.0)\n",
    "            return np.arccos(cos_sim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {self.metric}\")\n",
    "\n",
    "    def _initialize_centroids(self, X, rng):\n",
    "        \"\"\"Efficient k-means++ initialization.\"\"\"\n",
    "        centroids = [X[rng.randint(X.shape[0])]]\n",
    "        closest_dist_sq = self.calculate_distances(X, np.array(centroids))[:, 0] ** 2\n",
    "\n",
    "        for _ in range(1, self.n_clusters):\n",
    "            probs = closest_dist_sq / closest_dist_sq.sum()\n",
    "            cumulative_probs = np.cumsum(probs)\n",
    "            r = rng.rand()\n",
    "            new_centroid = X[np.searchsorted(cumulative_probs, r)]\n",
    "            centroids.append(new_centroid)\n",
    "            new_dist_sq = self.calculate_distances(X, np.array([new_centroid]))[:, 0] ** 2\n",
    "            closest_dist_sq = np.minimum(closest_dist_sq, new_dist_sq)\n",
    "        \n",
    "        return np.array(centroids)\n",
    "\n",
    "    @staticmethod\n",
    "    @njit\n",
    "    def _update_centroids(X, labels, n_clusters):\n",
    "        \"\"\"Compute new centroids using JIT compilation for efficiency.\"\"\"\n",
    "        new_centroids = np.zeros((n_clusters, X.shape[1]), dtype=X.dtype)\n",
    "        counts = np.zeros(n_clusters, dtype=np.int64)\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            new_centroids[labels[i]] += X[i]\n",
    "            counts[labels[i]] += 1\n",
    "        \n",
    "        for j in range(n_clusters):\n",
    "            if counts[j] > 0:\n",
    "                new_centroids[j] /= counts[j]\n",
    "        \n",
    "        return new_centroids\n",
    "\n",
    "    def _run_kmeans(self, X, rng):\n",
    "        \"\"\"Run a single instance of K-means clustering with optional mini-batch.\"\"\"\n",
    "        centroids = self._initialize_centroids(X, rng)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            if self.batch_size:\n",
    "                batch_indices = rng.choice(n_samples, self.batch_size, replace=False)\n",
    "                X_batch = X[batch_indices]\n",
    "                distances = self.calculate_distances(X_batch, centroids)\n",
    "                labels = np.argmin(distances, axis=1)\n",
    "            else:\n",
    "                distances = self.calculate_distances(X, centroids)\n",
    "                labels = np.argmin(distances, axis=1)\n",
    "            \n",
    "            new_centroids = self._update_centroids(X, labels, self.n_clusters)\n",
    "            \n",
    "            # Convergence check based on relative tolerance\n",
    "            centroid_shifts = np.linalg.norm(new_centroids - centroids, axis=1)\n",
    "            if np.all(centroid_shifts < self.tol * np.linalg.norm(centroids, axis=1)):\n",
    "                break\n",
    "            \n",
    "            centroids = new_centroids\n",
    "        \n",
    "        # Inertia calculation for this run\n",
    "        inertia = np.sum(np.min(distances, axis=1) ** 2)\n",
    "        return centroids, labels, inertia\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Run KMeans with multiple initializations to get the best clustering.\"\"\"\n",
    "        best_inertia = np.inf\n",
    "        best_centroids = None\n",
    "        best_labels = None\n",
    "        rng = check_random_state(self.random_state)\n",
    "\n",
    "        for _ in range(self.n_init):\n",
    "            centroids, labels, inertia = self._run_kmeans(X, rng)\n",
    "            \n",
    "            if inertia < best_inertia:\n",
    "                best_inertia = inertia\n",
    "                best_centroids = centroids\n",
    "                best_labels = labels\n",
    "        \n",
    "        # Set final results\n",
    "        self.centroids_ = best_centroids\n",
    "        self.labels_ = best_labels\n",
    "        self.inertia_ = best_inertia\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the closest cluster each sample in X belongs to.\"\"\"\n",
    "        distances = self.calculate_distances(X, self.centroids_)\n",
    "        return np.argmin(distances, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "40890922-2ee7-4442-9056-3bdafe4d0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_method_values(net, community_table, emb, score_keys):\n",
    "    # Normalize the vector of each node to have unit length. This normalization improves clustering.\n",
    "    #X = np.einsum(\"ij,i->ij\", emb, 1 / np.maximum(np.linalg.norm(emb, axis=1), 1e-24))\n",
    "    #X = emb.copy()\n",
    "\n",
    "    def method_score(key): \n",
    "        if key == \"kmeans++\":\n",
    "            kmeans = KMeans(n_clusters=len(set(community_table[\"community_id\"])), init='k-means++').fit(emb)\n",
    "            return calc_esim(community_table[\"community_id\"], kmeans.labels_)\n",
    "\n",
    "        elif key.startswith(\"kmeans_\"):  # Parse metric and apply CustomKMeans with modified metrics\n",
    "            metric = key.split(\"_\", 1)[1]\n",
    "            custom_kmeans = CustomKMeans(n_clusters=len(set(community_table[\"community_id\"])), metric=metric).fit(emb)\n",
    "            return calc_esim(community_table[\"community_id\"], custom_kmeans.labels_)\n",
    "\n",
    "    \n",
    "    # Calculate and store scores for each clustering method in score_keys\n",
    "    score_dictionary = {}\n",
    "    for key in score_keys:\n",
    "        score_dictionary[key] = method_score(key)\n",
    "\n",
    "    return score_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c2725-0e3c-4f0a-a581-ae4acee14ecb",
   "metadata": {},
   "source": [
    "Before we parallelize and get results for all the LFR networks of varying mixing rates and varying network densities. We will start with an example of just one network. We load the network, community information, embeddings, and run the modified K-Means using Dot, Euclidean and Cosine similarities on the embedding vectors generated using Node2Vec using Dot, Euclidean and Cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "936a0fbe-f3a8-443d-aa04-847369af414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net_and_embedding(net_filename, comm_filename, emb_filename):\n",
    "    net = sparse.load_npz(net_filename)\n",
    "    community_table = pd.read_csv(comm_filename)\n",
    "    \n",
    "    with open(emb_filename, 'rb') as f:  # open a text file\n",
    "        emb_dict = pickle.load(f) # deserialize using load()\n",
    "\n",
    "    return net, community_table, emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a4f5e4b-38c6-49f9-9a40-cd69cb7c04de",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "mu_values = np.round(np.arange(0.05, 1.05, 0.05),decimals=2)\n",
    "\n",
    "params = {\n",
    "    \"N\": N,\n",
    "    \"k\": 5,\n",
    "    \"maxk\":  int(np.sqrt(10 * N)),\n",
    "    \"minc\": 50,\n",
    "    \"maxc\": int(np.ceil(np.sqrt(N * 10))),\n",
    "    \"tau\": 3.0,\n",
    "    \"tau2\": 1.0,\n",
    "    \"mu\": 0.2,\n",
    "    }\n",
    "\n",
    "\n",
    "emb_params = {\n",
    "    \"method\": \"node2vec\",\n",
    "    \"window_length\": 10,\n",
    "    \"walk_length\": 80,\n",
    "    \"num_walks\": 10,\n",
    "    \"dim\": 64,\n",
    "}\n",
    "\n",
    "\n",
    "k=5 # k = {5,10,50}\n",
    "mu = 0.1\n",
    "run_no = 1\n",
    "\n",
    "path_name = f\"/nobackup/gogandhi/alt_means_sans_k/data/experiment_n2v_metric_change_10000_{k}_3.0_minc50/Run_{run_no}/\" \n",
    "\n",
    "net_filename = path_name + f\"net_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{k}_mincomm_50.npz\"  # A = sp.load_npz(net_path)\n",
    "comm_filename = path_name + f\"community_table_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{k}_mincomm_50.csv\" # pd.read_csv()\n",
    "emb_filename = path_name + f\"embeddings_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{k}_mincomm_50.pkl\" # embeddings_dict\n",
    "\n",
    "#\"community_table_LFR_n_10000_tau1_3.0_tau2_1.0_mu_0.1_k_50_mincomm_50.npz\"\n",
    "\n",
    "net, community_table, emb_dict = load_net_and_embedding(net_filename, comm_filename, emb_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25a75791-2f73-489b-a592-81d12a2d7f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kmeans++': 0.8883255562176959, 'kmeans_euclidean': 0.9190984190926753, 'kmeans_dot': 0.676045991603713, 'kmeans_cosine': 0.8216621800325635}\n",
      "{'kmeans++': 0.9925325345751824, 'kmeans_euclidean': 0.9925325345751824, 'kmeans_dot': 0.6464478510220129, 'kmeans_cosine': 0.9903730792768748}\n",
      "{'kmeans++': 0.9478108231369303, 'kmeans_euclidean': 0.9385970563145652, 'kmeans_dot': 0.7824215655820644, 'kmeans_cosine': 0.9515917299553007}\n"
     ]
    }
   ],
   "source": [
    "score_keys=['kmeans++','kmeans_euclidean','kmeans_dot','kmeans_cosine'] \n",
    "for key in ['dot', 'euclidean', 'cosine']:\n",
    "    emb = emb_dict[key]\n",
    "    print(clustering_method_values(net, community_table, emb, score_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009c8e3-af82-4f5a-a4f3-79450d5a1dc7",
   "metadata": {},
   "source": [
    "We go through 20 iterations of Kmeans, and use the best clustering of the them. The Kmeans++ (which is the standard optimized implementation) performs comparable to our Kmeans_euclidean which is our baseline for our modified version especially in the euclidean-euclidean case. When embeddings are generated using other methods, it does falter a bit. This is exciting news! Which means there could be more to uncover!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e3eef-1b75-47e3-8e83-533f3a6bdebf",
   "metadata": {},
   "source": [
    "# Parallelization to get clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "301bd635-1569-4f54-8eaa-eded78c7f37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-23:\n",
      "Process ForkProcess-24:\n",
      "Process ForkProcess-29:\n",
      "Process ForkProcess-28:\n",
      "Process ForkProcess-30:\n",
      "Process ForkProcess-22:\n",
      "Process ForkProcess-26:\n",
      "Process ForkProcess-25:\n",
      "Process ForkProcess-27:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll combinations processed in parallel. Total elapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_elapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mprocess_all_combinations_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 77\u001b[0m, in \u001b[0;36mprocess_all_combinations_parallel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(process_run, run_no, mu): (run_no, mu) \n\u001b[1;32m     74\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m run_no, mu \u001b[38;5;129;01min\u001b[39;00m runs_mu_combinations}\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# As each future completes, collect its results\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(as_completed(futures), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     78\u001b[0m     run_no, mu \u001b[38;5;241m=\u001b[39m futures[future]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/nobackup/gogandhi/miniconda3/envs/gensim_mod_env/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import torch\n",
    "\n",
    "# Assuming these functions are defined elsewhere\n",
    "# from your_module import load_net_and_embedding, clustering_method_values\n",
    "\n",
    "# Parameters\n",
    "N = 10000\n",
    "K = 50\n",
    "mu_values = np.round(np.arange(0.05, 1.05, 0.05), decimals=2)\n",
    "params_template = {\n",
    "    \"N\": N,\n",
    "    \"k\": K,\n",
    "    \"maxk\": int(np.sqrt(10 * N)),\n",
    "    \"minc\": 50,\n",
    "    \"maxc\": int(np.ceil(np.sqrt(N * 10))),\n",
    "    \"tau\": 3.0,\n",
    "    \"tau2\": 1.0,\n",
    "}\n",
    "emb_params = {\n",
    "    \"method\": \"node2vec\",\n",
    "    \"window_length\": 10,\n",
    "    \"walk_length\": 80,\n",
    "    \"num_walks\": 10,\n",
    "    \"dim\": 64,\n",
    "}\n",
    "\n",
    "score_keys = ['kmeans++', 'kmeans_euclidean', 'kmeans_dot', 'kmeans_cosine']\n",
    "\n",
    "# Output directory â€“ we will create one file per embedding type\n",
    "output_dir = f\"/nobackup/gogandhi/alt_means_sans_k/data/experiment_n2v_metric_change_10000_{K}_3.0_minc50/\"\n",
    "\n",
    "\n",
    "# Function to process a single run and mu value\n",
    "def process_run(run_no, mu):\n",
    "    # Build file paths based on run and mu\n",
    "    \n",
    "    path_name = f\"/nobackup/gogandhi/alt_means_sans_k/data/experiment_n2v_metric_change_10000_{K}_3.0_minc50/Run_{run_no}/\"\n",
    "    net_filename = path_name + f\"net_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{K}_mincomm_50.npz\"\n",
    "    comm_filename = path_name + f\"community_table_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{K}_mincomm_50.csv\"\n",
    "    emb_filename = path_name + f\"embeddings_LFR_n_10000_tau1_3.0_tau2_1.0_mu_{mu}_k_{K}_mincomm_50.pkl\"\n",
    "    \n",
    "    # Load network, community table, and the embedding dictionary\n",
    "    net, community_table, emb_dict = load_net_and_embedding(net_filename, comm_filename, emb_filename)\n",
    "    \n",
    "    # For each embedding in the dictionary, run clustering and prepare a result string\n",
    "    results = []\n",
    "    for emb_key, emb in emb_dict.items():\n",
    "        result = clustering_method_values(net, community_table, emb, score_keys)\n",
    "        result_values = [result[key] for key in score_keys]\n",
    "        # Format: run_no,mu,score1,score2,...\n",
    "        result_str = f\"{run_no},{mu},\" + \",\".join(map(str, result_values))\n",
    "        results.append((emb_key, result_str))\n",
    "        print(f\"Completed Run {run_no} with Mu {mu} for embedding '{emb_key}'\")\n",
    "    return results\n",
    "\n",
    "# Function to process all run/mu combinations in parallel\n",
    "def process_all_combinations_parallel():\n",
    "    # Generate all combinations of run numbers and mu values\n",
    "    runs_mu_combinations = [(run_no, mu) for run_no in range(1, 2) for mu in [0.1]]\n",
    "    total_combinations = len(runs_mu_combinations)\n",
    "    start_time = time.time()\n",
    "    #print(total_combinations)\n",
    "    # Dictionary to collect results per embedding type\n",
    "    all_results = {}  # {embedding_key: [result_str, ...]}\n",
    "    \n",
    "    # Parallel processing using ProcessPoolExecutor\n",
    "    with ProcessPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(process_run, run_no, mu): (run_no, mu) \n",
    "                   for run_no, mu in runs_mu_combinations}\n",
    "        \n",
    "        # As each future completes, collect its results\n",
    "        for idx, future in enumerate(as_completed(futures), start=1):\n",
    "            run_no, mu = futures[future]\n",
    "            try:\n",
    "                run_results = future.result()  # List of (emb_key, result_str) tuples\n",
    "                for emb_key, result_str in run_results:\n",
    "                    if emb_key not in all_results:\n",
    "                        all_results[emb_key] = []\n",
    "                    all_results[emb_key].append(result_str)\n",
    "                print(f\"Processed {idx}/{total_combinations} combinations: Run {run_no}, Mu {mu}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Run {run_no}, Mu {mu}: {e}\")\n",
    "    \n",
    "    # Write out separate output files per embedding type\n",
    "    for emb_key, results in all_results.items():\n",
    "        output_file = os.path.join(output_dir, f\"n2v_{emb_key}_kmeans_clustering.txt\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            header = \"run_no,mu,\" + \",\".join(score_keys) + \"\\n\"\n",
    "            f.write(header)\n",
    "            for line in results:\n",
    "                f.write(line + \"\\n\")\n",
    "        print(f\"Results written to {output_file}\")\n",
    "    \n",
    "    total_elapsed_time = time.time() - start_time\n",
    "    print(f\"All combinations processed in parallel. Total elapsed time: {total_elapsed_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_combinations_parallel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1a08a9c-77dc-4916-b200-a2c3a4d1e893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Run 1 with Mu 0.1 for embedding 'dot'\n",
      "Completed Run 1 with Mu 0.1 for embedding 'euclidean'\n",
      "Completed Run 1 with Mu 0.1 for embedding 'cosine'\n",
      "Direct call results: [('dot', '1,0.1,0.930774835825718,0.9078791558988919,0.8109024487542578,0.8850640546357088'), ('euclidean', '1,0.1,1.0,1.0,0.47745189633168655,1.0'), ('cosine', '1,0.1,0.9577367930235333,0.9694957803036819,0.7632839896711154,1.0')]\n",
      "72.08514332771301\n"
     ]
    }
   ],
   "source": [
    "# Instead of using ProcessPoolExecutor, run the function directly:\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "results = process_run(1, 0.1)\n",
    "print(\"Direct call results:\", results)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2160f34b-e176-413c-b994-3263a5309472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import gc\n",
    "\n",
    "# Invoke garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ff080-f2fe-4871-af0f-9effed09e59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim_mod_env",
   "language": "python",
   "name": "gensim_mod_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
